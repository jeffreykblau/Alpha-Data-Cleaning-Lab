# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import sqlite3
import os
from MarketRuleRouter import MarketRuleRouter

class AlphaCoreEngine:
    """
    Alpha æ ¸å¿ƒæ•¸æ“šç²¾ç…‰å¼•æ“
    åŠŸèƒ½ï¼šæ•´åˆå¤šåœ‹å¸‚å ´è¦å‰‡ï¼Œè¨ˆç®—æ‰€æœ‰å‰ç«¯é é¢æ‰€éœ€çš„å‹•èƒ½ã€é¢¨éšªã€é€£æ¿èˆ‡é æ¸¬æŒ‡æ¨™ã€‚
    """
    def __init__(self, db_path, market_abbr):
        self.db_path = db_path
        self.market_abbr = market_abbr
        self.conn = sqlite3.connect(db_path)
        self.rules = MarketRuleRouter.get_rules(market_abbr)
        self.df = None

    def load_data(self):
        """ å¾åŸå§‹è¡¨è¼‰å…¥æ•¸æ“š """
        query = "SELECT * FROM daily_stock_data"
        self.df = pd.read_sql(query, self.conn)
        self.df['æ—¥æœŸ'] = pd.to_datetime(self.df['æ—¥æœŸ'])
        return self

    def execute_full_pipeline(self):
        """ åŸ·è¡Œå®Œæ•´ç²¾ç…‰æµç¨‹ï¼Œç¢ºä¿æ‰€æœ‰é é¢æ¬„ä½è£œé½Š """
        if self.df is None:
            self.load_data()

        print(f"ğŸš€ é–‹å§‹ç²¾ç…‰ {self.market_abbr} å¸‚å ´æ•¸æ“š...")

        # 1. æ‡‰ç”¨å¸‚å ´è¦å‰‡ (å…§å«ï¼šä¹’ä¹“æ¸…æ´—ã€is_limit_up æ¨™è¨»ã€failed_lu_threshold)
        self.df = self.rules.apply(self.df)

        # 2. è¨ˆç®—åŸºç¤å›å ±èˆ‡ Deep_Scan / Today_Limit_Up æ‰€éœ€çš„é æ¸¬æ¬„ä½
        self._calculate_core_returns()

        # 3. è¨ˆç®—é€£æ¿å¤©æ•¸ (æ”¯æ´ Today_Limit_Up çš„ Seq_LU_Count)
        self._calculate_sequence_counts()

        # 4. è¨ˆç®—æ»¾å‹•å‹•èƒ½èˆ‡å‘¨æœŸå›å ± (æ”¯æ´ Period_Analysis çš„ç¹ç°¡é«”å‘½åéœ€æ±‚)
        self._calculate_period_metrics()

        # 5. è¨ˆç®—é¢¨éšªæŒ‡æ¨™ (æ”¯æ´ Risk_Metrics çš„ Volatility èˆ‡ Drawdown)
        self._calculate_risk_metrics()

        # 6. æœ€çµ‚æ•´ç†èˆ‡å­˜æª”
        self._save_to_db()
        return f"âœ… {self.market_abbr} æ•¸æ“šç²¾ç…‰å®Œæˆï¼Œå·²å­˜å…¥ cleaned_daily_baseã€‚"

    def _calculate_core_returns(self):
        """ è¨ˆç®—åŸºç¤å ±é…¬ç‡èˆ‡éš”æ—¥æº¢åƒ¹ã€éš”æ—¥ç©ºé–“ """
        self.df = self.df.sort_values(['StockID', 'æ—¥æœŸ'])
        groups = self.df.groupby('StockID')

        # ä»Šæ—¥å ±é…¬
        self.df['Ret_Day'] = (self.df['æ”¶ç›¤'] / self.df['Prev_Close']) - 1
        # ä»Šæ—¥ç›¤ä¸­æœ€é«˜æ¼²å¹… (ç”¨ä¾†åˆ¤å®šç‚¸æ¿)
        self.df['Ret_High'] = (self.df['æœ€é«˜'] / self.df['Prev_Close']) - 1
        # ä»Šæ—¥éš”å¤œæº¢åƒ¹ (é–‹ç›¤ç›¸å°æ–¼æ˜¨æ”¶)
        self.df['Overnight_Alpha'] = (self.df['é–‹ç›¤'] / self.df['Prev_Close']) - 1
        
        # å»ºç«‹ Prev_LU (æ˜¨æ—¥æ˜¯å¦æ¼²åœ)
        self.df['Prev_LU'] = groups['is_limit_up'].shift(1).fillna(0)
        
        # å»ºç«‹ Next_1D_Max (æ˜æ—¥æœ€å¤§ç©ºé–“ - ä¾› Deep_Scan è¨ºæ–·æ˜æ—¥å‹ç‡)
        self.df['Next_1D_Max'] = groups['Ret_High'].shift(-1)
        
        # å»ºç«‹ Next_1D_Ret (æ˜æ—¥çµ‚å ´æ¼²è·Œ)
        self.df['Next_1D_Ret'] = groups['Ret_Day'].shift(-1)

    def _calculate_sequence_counts(self):
        """ è¨ˆç®—é€£çºŒæ¼²åœå¤©æ•¸ (Seq_LU_Count) """
        def get_seq_lu(s):
            # é€é block ç´¯ç©ä¾†å€åˆ†é€£çºŒå€å¡Š
            blocks = (s != s.shift()).cumsum()
            return (s == 1).astype(int) * (s.groupby(blocks).cumcount() + 1)
        
        self.df['Seq_LU_Count'] = self.df.groupby('StockID')['is_limit_up'].transform(get_seq_lu)

    def _calculate_period_metrics(self):
        """ è¨ˆç®—æ»¾å‹•å ±é…¬èˆ‡ç‰¹å®šé€±æœŸå ±é…¬ (å°æ¥ Period_Analysis) """
        groups = self.df.groupby('StockID')

        # A. æ»¾å‹•å›å ± (5D, 20D, 200D)
        for d in [5, 20, 200]:
            self.df[f'Ret_{d}D'] = groups['æ”¶ç›¤'].transform(lambda x: x.pct_change(periods=d))

        # B. æ—¥æ›†é€±æœŸå ±é…¬ (æ¡ç”¨é é¢è¦æ±‚çš„ç‰¹å®šç¹ç°¡é«”å‘½å)
        # æœ¬å‘¨ç´¯ç©ï¼šå¾æœ¬å‘¨ç¬¬ä¸€å€‹äº¤æ˜“æ—¥è‡³ä»Š
        self.df['å‘¨ç´¯è®¡æ¼²è·Œå¹…(æœ¬å‘¨å¼€ç›˜)'] = groups['æ”¶ç›¤'].transform(
            lambda x: x / x.rolling(window=5, min_periods=1).apply(lambda y: y[0], raw=True) - 1
        )
        # æœ¬æœˆç´¯ç©
        self.df['æœˆç´¯è®¡æ¼²è·Œå¹…(æœ¬æœˆå¼€ç›˜)'] = groups['æ”¶ç›¤'].transform(
            lambda x: x / x.rolling(window=20, min_periods=1).apply(lambda y: y[0], raw=True) - 1
        )
        # æœ¬å¹´ç´¯ç©
        self.df['å¹´ç´¯è¨ˆæ¼²è·Œå¹…(æœ¬å¹´å¼€ç›˜)'] = groups['æ”¶ç›¤'].transform(
            lambda x: x / x.rolling(window=250, min_periods=1).apply(lambda y: y[0], raw=True) - 1
        )

    def _calculate_risk_metrics(self):
        """ è¨ˆç®—é¢¨éšªæŒ‡æ¨™ (å°æ¥ Risk_Metrics) """
        groups = self.df.groupby('StockID')

        # 1. æ»¾å‹•æ³¢å‹•ç‡ (Volatility - å¹´åŒ–æ¨™æº–å·®)
        for d in [10, 20, 50]:
            self.df[f'volatility_{d}d'] = groups['Ret_Day'].transform(
                lambda x: x.rolling(window=d).std() * np.sqrt(252)
            )

        # 2. æœ€å¤§å›æ’¤ (Drawdown after High)
        # é‚è¼¯ï¼š(ä»Šæ—¥æ”¶ç›¤ / è¿‘ N æ—¥æœ€é«˜åƒ¹) - 1
        for d in [10, 20, 50]:
            rolling_high = groups['æœ€é«˜'].transform(lambda x: x.rolling(window=d, min_periods=1).max())
            self.df[f'drawdown_after_high_{d}d'] = (self.df['æ”¶ç›¤'] / rolling_high) - 1

        # 3. æ¢å¾©åŠ› (Recovery)
        # ç°¡åŒ–é‚è¼¯ï¼šä»Šæ—¥æ”¶ç›¤ç›¸å°æ–¼ 10D æœ€ä½é»çš„å›å‡å¹…åº¦
        rolling_low_10 = groups['æœ€ä½'].transform(lambda x: x.rolling(window=10, min_periods=1).min())
        self.df['recovery_from_dd_10d'] = (self.df['æ”¶ç›¤'] / rolling_low_10) - 1

    def _save_to_db(self):
        """ å„²å­˜ç²¾ç…‰å¾Œçš„æ•¸æ“š """
        # è½‰æ›æ—¥æœŸæ ¼å¼ä»¥ä¾¿ SQLite æ’åº
        save_df = self.df.copy()
        save_df['æ—¥æœŸ'] = save_df['æ—¥æœŸ'].dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # å­˜å…¥æ–°è¡¨
        save_df.to_sql("cleaned_daily_base", self.conn, if_exists="replace", index=False)
        self.conn.close()

# --- ä½¿ç”¨ç¯„ä¾‹ ---
# engine = AlphaCoreEngine("tw_stock_warehouse.db", "TW")
# engine.execute_full_pipeline()
