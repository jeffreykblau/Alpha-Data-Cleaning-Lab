{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grissomlin/Alpha-Data-Cleaning-Lab/blob/main/%E3%80%8C%E8%B2%A1%E5%A0%B1%E5%84%80%E8%A1%A8%E6%9D%BF%E7%9B%B8%E9%97%9C%E7%A8%8B%E5%BC%8F%E7%A2%BC%E3%80%8D_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ğŸ‡¹ğŸ‡¼ Taiwan Stock Price Local Refinery (2020â€“2025)\n",
        "# ğŸ‡ºğŸ‡¸ å°ç£å…¨å¸‚å ´è‚¡åƒ¹æœ¬åœ°ç²¾ç…‰å™¨ï¼ˆ2020â€“2025ï¼‰\n",
        "#\n",
        "# ğŸ” Purpose / ç›®çš„ï¼š\n",
        "#   Automatically crawl the complete list of Taiwanese securities (listed, OTC, ETFs, DRs, etc.)\n",
        "#   from TWSE/TPEx official ISIN pages, then download their historical OHLCV prices via yfinance,\n",
        "#   and store them in a local SQLite database with clean structure.\n",
        "#\n",
        "#   è‡ªå‹•å¾å°ç£è­‰äº¤æ‰€ï¼æ«ƒè²·ä¸­å¿ƒå®˜æ–¹ ISIN é é¢æŠ“å–å…¨å¸‚å ´æœ‰åƒ¹è­‰åˆ¸æ¸…å–®ï¼ˆä¸Šå¸‚ã€ä¸Šæ«ƒã€ETFã€TDR ç­‰ï¼‰ï¼Œ\n",
        "#   é€é yfinance ä¸‹è¼‰ 2020 å¹´è‡³ä»Šçš„æ­·å²é–‹é«˜ä½æ”¶èˆ‡æˆäº¤é‡ï¼Œä¸¦å„²å­˜è‡³æœ¬åœ° SQLite è³‡æ–™åº«ã€‚\n",
        "#\n",
        "# ğŸ—‚ï¸ Output / è¼¸å‡ºï¼š\n",
        "#   - A single SQLite file: `local_taiwan_stock.db`\n",
        "#     Contains table `stock_prices` with columns: date, symbol, open, high, low, close, volume\n",
        "#   - Random sample of latest trading day for quick validation\n",
        "#\n",
        "#   - å–®ä¸€ SQLite æª”æ¡ˆï¼š`local_taiwan_stock.db`\n",
        "#     åŒ…å«è³‡æ–™è¡¨ `stock_prices`ï¼Œæ¬„ä½ï¼šdate, symbol, open, high, low, close, volume\n",
        "#   - è‡ªå‹•æŠ½æ¨£æœ€æ–°äº¤æ˜“æ—¥ 10 ç­†è³‡æ–™ä¾›å¿«é€Ÿé©—è­‰\n",
        "#\n",
        "# âš ï¸ Important Notes / æ³¨æ„äº‹é …ï¼š\n",
        "#   - This script OVERWRITES the existing DB on every run (for reproducibility).\n",
        "#   - Uses raw (unadjusted) prices to preserve true OHLC structure; adjust later if needed.\n",
        "#   - Single-threaded download to avoid IP ban; includes polite delays.\n",
        "#   - Symbols are suffixed with .TW (listed) or .TWO (OTC/ROTC) for yfinance compatibility.\n",
        "#\n",
        "#   - æ¯æ¬¡åŸ·è¡Œæœƒè¦†è“‹ç¾æœ‰ DBï¼ˆç¢ºä¿å¯é‡ç¾æ€§ï¼‰ã€‚\n",
        "#   - ä½¿ç”¨æœªè¤‡æ¬ŠåŸå§‹åƒ¹æ ¼ï¼ˆä¿ç•™çœŸå¯¦ K ç·šçµæ§‹ï¼‰ï¼Œå¾ŒçºŒå¦‚éœ€è¤‡æ¬Šè«‹å¦è¡Œè™•ç†ã€‚\n",
        "#   - å–®åŸ·è¡Œç·’ä¸‹è¼‰é¿å…è¢«å° IPï¼Œä¸¦åŠ å…¥ç¦®è²Œæ€§å»¶é²ã€‚\n",
        "#   - è‚¡ç¥¨ä»£è™Ÿå·²è‡ªå‹•é™„åŠ  .TWï¼ˆä¸Šå¸‚ï¼‰æˆ– .TWOï¼ˆä¸Šæ«ƒï¼èˆˆæ«ƒï¼‰ä»¥ç¬¦åˆ yfinance æ ¼å¼ã€‚\n",
        "#\n",
        "# âœ… Use Case / é©ç”¨å ´æ™¯ï¼š\n",
        "#   - Building a clean local dataset for backtesting, research, or ETL pipelines.\n",
        "#   - Preparing data before uploading to cloud databases (e.g., Supabase).\n",
        "#\n",
        "#   - ç‚ºå›æ¸¬ã€ç ”ç©¶æˆ– ETL æµç¨‹å»ºç«‹ä¹¾æ·¨çš„æœ¬åœ°è³‡æ–™é›†ã€‚\n",
        "#   - ä¸Šå‚³è‡³é›²ç«¯è³‡æ–™åº«ï¼ˆå¦‚ Supabaseï¼‰å‰çš„è³‡æ–™æº–å‚™éšæ®µã€‚\n",
        "# =============================================================================\n",
        "\n",
        "# 1. å®‰è£å¿…è¦å¥—ä»¶\n",
        "!pip install yfinance tqdm pandas requests lxml -q\n",
        "\n",
        "import os, time, requests, pandas as pd, yfinance as yf\n",
        "import sqlite3\n",
        "from io import StringIO\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ========== æ ¸å¿ƒé…ç½®ï¼šä½ çš„ 7 çµ„å¸‚å ´è¨­å®š ==========\n",
        "url_configs = [\n",
        "    {'name': 'listed', 'url': 'https://isin.twse.com.tw/isin/class_main.jsp?market=1&issuetype=1&Page=1&chklike=Y', 'suffix': '.TW'},\n",
        "    {'name': 'dr', 'url': 'https://isin.twse.com.tw/isin/class_main.jsp?owncode=&stockname=&isincode=&market=1&issuetype=J&industry_code=&Page=1&chklike=Y', 'suffix': '.TW'},\n",
        "    {'name': 'otc', 'url': 'https://isin.twse.com.tw/isin/class_main.jsp?market=2&issuetype=4&Page=1&chklike=Y', 'suffix': '.TWO'},\n",
        "    {'name': 'etf', 'url': 'https://isin.twse.com.tw/isin/class_main.jsp?owncode=&stockname=&isincode=&market=1&issuetype=I&industry_code=&Page=1&chklike=Y', 'suffix': '.TW'},\n",
        "    {'name': 'rotc', 'url': 'https://isin.twse.com.tw/isin/class_main.jsp?owncode=&stockname=&isincode=&market=E&issuetype=R&industry_code=&Page=1&chklike=Y', 'suffix': '.TWO'},\n",
        "    {'name': 'tw_innovation', 'url': 'https://isin.twse.com.tw/isin/class_main.jsp?owncode=&stockname=&isincode=&market=C&issuetype=C&industry_code=&Page=1&chklike=Y', 'suffix': '.TW'},\n",
        "    {'name': 'otc_innovation', 'url': 'https://isin.twse.com.tw/isin/class_main.jsp?owncode=&stockname=&isincode=&market=A&issuetype=C&industry_code=&Page=1&chklike=Y', 'suffix': '.TWO'},\n",
        "]\n",
        "\n",
        "DB_NAME = \"local_taiwan_stock.db\"\n",
        "START_DATE = \"2020-01-01\"\n",
        "\n",
        "def log(msg: str):\n",
        "    print(f\"{datetime.now().strftime('%H:%M:%S')}: {msg}\")\n",
        "\n",
        "# ========== ç¬¬ä¸€éšæ®µï¼šåˆå§‹åŒ–æœ¬åœ°è³‡æ–™åº« ==========\n",
        "if os.path.exists(DB_NAME): os.remove(DB_NAME) # æ¯æ¬¡åŸ·è¡Œéƒ½æ¸…ç©ºé‡ä¾†ï¼Œä¿è­‰ä¹¾æ·¨\n",
        "conn = sqlite3.connect(DB_NAME)\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"\"\"\n",
        "    CREATE TABLE stock_prices (\n",
        "        date TEXT,\n",
        "        symbol TEXT,\n",
        "        open REAL,\n",
        "        high REAL,\n",
        "        low REAL,\n",
        "        close REAL,\n",
        "        volume INTEGER,\n",
        "        PRIMARY KEY (date, symbol)\n",
        "    )\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "\n",
        "try:\n",
        "    log(\"ğŸš€ å•Ÿå‹•æœ¬åœ°ç²¾ç…‰ä»»å‹™ (2020-2025)...\")\n",
        "\n",
        "    # ç¬¬äºŒéšæ®µï¼šç²å–å…¨å¸‚å ´æ¸…å–®\n",
        "    all_stocks = []\n",
        "    for cfg in url_configs:\n",
        "        log(f\"ğŸ“¡ æŠ“å– {cfg['name']} åŸå§‹æ¸…å–®...\")\n",
        "        try:\n",
        "            resp = requests.get(cfg['url'], timeout=15)\n",
        "            df = pd.read_html(StringIO(resp.text), header=0)[0]\n",
        "            for _, row in df.iterrows():\n",
        "                code = str(row.get('æœ‰åƒ¹è­‰åˆ¸ä»£è™Ÿ', '')).strip()\n",
        "                if code.isalnum() and len(code) >= 4:\n",
        "                    all_stocks.append((f\"{code}{cfg['suffix']}\", str(row.get('æœ‰åƒ¹è­‰åˆ¸åç¨±', '')).strip()))\n",
        "        except Exception as e:\n",
        "            log(f\"âŒ {cfg['name']} çˆ¬å–å¤±æ•—: {e}\")\n",
        "\n",
        "    # ç¬¬ä¸‰éšæ®µï¼šå–®åŸ·è¡Œç·’é€ä¸€æŠ“å–ä¸¦å­˜å…¥ SQLite\n",
        "    log(f\"ğŸš€ é–‹å§‹ä¸‹è¼‰è‚¡åƒ¹ä¸¦å­˜å…¥æœ¬åœ° DBï¼Œé è¨ˆè™•ç† {len(all_stocks)} æª”...\")\n",
        "    success_count = 0\n",
        "\n",
        "    for symbol, name in tqdm(all_stocks, desc=\"ç²¾ç…‰é€²åº¦\"):\n",
        "        try:\n",
        "            # threads=False ç¢ºä¿æ•¸æ“šä¸è¡çªï¼Œauto_adjust=False ç¢ºä¿å››åƒ¹åŸå§‹æ­£ç¢º\n",
        "            data = yf.download(symbol, start=START_DATE, progress=False, threads=False, auto_adjust=False)\n",
        "            if data.empty: continue\n",
        "\n",
        "            # è™•ç†å¤šå±¤ç´¢å¼•\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "            data = data.reset_index()\n",
        "            data.columns = [c.lower() for c in data.columns]\n",
        "\n",
        "            # è³‡æ–™è½‰æ›\n",
        "            data['symbol'] = symbol\n",
        "            data['date'] = data['date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "            # åªå–å››åƒ¹ã€é‡èˆ‡æ—¥æœŸ\n",
        "            final_df = data[['date', 'symbol', 'open', 'high', 'low', 'close', 'volume']]\n",
        "\n",
        "            # å¯«å…¥ SQLite\n",
        "            final_df.to_sql('stock_prices', conn, if_exists='append', index=False)\n",
        "            success_count += 1\n",
        "\n",
        "            # æ¯ 20 æª”ç¨å¾®ä¼‘æ¯ï¼Œé˜²æ­¢ IP è¢«é–\n",
        "            if success_count % 20 == 0:\n",
        "                time.sleep(0.5)\n",
        "\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    conn.commit()\n",
        "    log(f\"âœ… ç²¾ç…‰å®Œæˆï¼æˆåŠŸå„²å­˜ {success_count} æª”å€‹è‚¡æ­·å²æ•¸æ“šã€‚\")\n",
        "\n",
        "    # ========== ç¬¬å››éšæ®µï¼šéš¨æ©ŸæŠ½æ¨£ 10 ç­†æœ€æ–°äº¤æ˜“æ—¥æ•¸æ“š ==========\n",
        "    log(\"ğŸ“Š æ­£åœ¨åŸ·è¡Œæ•¸æ“šæŠ½æ¨£æª¢æŸ¥...\")\n",
        "\n",
        "    # å…ˆæ‰¾å‡ºè³‡æ–™åº«ä¸­æœ€æ–°çš„äº¤æ˜“æ—¥\n",
        "    cursor.execute(\"SELECT MAX(date) FROM stock_prices\")\n",
        "    latest_date = cursor.fetchone()[0]\n",
        "\n",
        "    if latest_date:\n",
        "        query = f\"\"\"\n",
        "            SELECT date, symbol, open, high, low, close, volume\n",
        "            FROM stock_prices\n",
        "            WHERE date = '{latest_date}'\n",
        "            ORDER BY RANDOM()\n",
        "            LIMIT 10\n",
        "        \"\"\"\n",
        "        df_samples = pd.read_sql(query, conn)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"ğŸ“… æœ€æ–°äº¤æ˜“æ—¥ ({latest_date}) éš¨æ©Ÿ 10 ç­†æŠ½æ¨£æª¢æŸ¥ï¼š\")\n",
        "        print(\"=\"*80)\n",
        "        # æ ¼å¼åŒ–é¡¯ç¤ºåƒ¹æ ¼\n",
        "        pd.options.display.float_format = '{:.2f}'.format\n",
        "        print(df_samples.to_string(index=False))\n",
        "        print(\"=\"*80)\n",
        "        print(\"ğŸ’¡ è«‹æª¢æŸ¥ä¸Šè¿° [close] æ˜¯å¦èˆ‡çœ‹ç›¤è»Ÿé«”ä¸€è‡´ã€‚è‹¥æ­£ç¢ºï¼Œå³å¯å°‡ DB è½‰å‡ºç‚º CSVã€‚\")\n",
        "    else:\n",
        "        print(\"âŒ æŠ½æ¨£å¤±æ•—ï¼šè³‡æ–™åº«å…§ç„¡æœ‰æ•ˆæ—¥æœŸæ•¸æ“šã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    log(f\"âŒ åš´é‡éŒ¯èª¤: {e}\")\n",
        "finally:\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "w0d4Lm8BuGx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ğŸ“Š [Cell 0] å°ç£ä¸Šå¸‚å…¬å¸ & ä¸Šæ«ƒå…¬å¸ã€Œæœˆç‡Ÿæ”¶è³‡æ–™ã€è‡ªå‹•åŒ–æŠ“å–å·¥å…·\n",
        "# =============================================================================\n",
        "# ğŸ” åŠŸèƒ½èªªæ˜ï¼š\n",
        "#   - å¾å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ (MOPS) è‡ªå‹•æŠ“å–æœ€è¿‘ N å€‹æœˆçš„æœˆç‡Ÿæ”¶å ±è¡¨ï¼ˆå« SII/OTC/ROTC + KY è‚¡ï¼‰\n",
        "#   - æ”¯æ´ã€Œé›™ä¸»æ©Ÿ fallbackã€ï¼šmops.twse.com.twï¼ˆèˆŠï¼‰ + mopsov.twse.com.twï¼ˆæ–°ï¼‰\n",
        "#   - è‡ªå‹•è™•ç†ç·¨ç¢¼ï¼ˆBig5/UTF-8ï¼‰ã€è¡¨æ ¼è§£æã€æ¬„ä½æ¨™æº–åŒ–ã€æ•¸å€¼æ¸…æ´—ã€å»é‡\n",
        "#   - è¼¸å‡ºå…©ä»½ CSVï¼š\n",
        "#       1. å®Œæ•´åŸå§‹è³‡æ–™ï¼ˆå«ä¾†æºæ¨™è¨˜ï¼‰\n",
        "#       2. ï¼ˆå¯æ“´å±•ï¼‰æœªä¾†å¯ç”¨æ–¼ç”Ÿæˆ watchlist æˆ–ç•°å¸¸åµæ¸¬\n",
        "#\n",
        "# âš™ï¸ æ ¸å¿ƒç‰¹è‰²ï¼š\n",
        "#   âœ… æ°‘åœ‹å¹´æœˆ â†’ è¥¿å…ƒå¹´æœˆ è‡ªå‹•è½‰æ›\n",
        "#   âœ… å…­é¡å ±è¡¨åˆä½µï¼šä¸Šå¸‚(SII) / ä¸Šæ«ƒ(OTC) / èˆˆæ«ƒ(ROTC) Ã— ä¸€èˆ¬è‚¡/KYè‚¡\n",
        "#   âœ… æ¬„ä½æ¨¡ç³ŠåŒ¹é… + æ­£å‰‡æ¨™æº–åŒ–ï¼ˆå®¹å¿ç©ºæ ¼ã€å…¨å½¢ã€åˆ¥åï¼‰\n",
        "#   âœ… æ•¸å€¼æ¸…æ´—ï¼šè™•ç†ã€Œï¼ã€ã€ã€Œ--ã€ã€ã€Œ,ã€ç­‰éæ¨™æº–æ ¼å¼\n",
        "#   âœ… å¤šç·šç¨‹åŠ é€Ÿï¼ˆThreadPoolExecutorï¼‰+ æŒ‡æ•¸é€€é¿é‡è©¦æ©Ÿåˆ¶\n",
        "#   âœ… åš´æ ¼éæ¿¾ã€Œåˆè¨ˆè¡Œã€èˆ‡ã€Œç„¡æ•ˆä»£è™Ÿã€\n",
        "#   âœ… è‡ªå‹•å»é‡ï¼šåŒæœˆåŒæ¿å¡ŠåŒè‚¡ç¥¨ä»£è™Ÿåªä¿ç•™ç¬¬ä¸€ç­†\n",
        "#\n",
        "# ğŸ“… æ™‚é–“ç¯„åœæ§åˆ¶ï¼š\n",
        "#   - ä»¥ END_GREG_YM ç‚ºçµ‚é»ï¼ˆè¥¿å…ƒ YYYY-MMï¼‰ï¼Œå¾€å›æŠ“ MONTHS_TO_FETCH å€‹æœˆ\n",
        "#   - é è¨­æŠ“ 80 å€‹æœˆ â‰ˆ 6.5 å¹´ï¼ˆæ¶µè“‹ 2019â€“2025ï¼‰ï¼Œç¢ºä¿åŒ…å«ç–«æƒ…å‰å¾Œå®Œæ•´é€±æœŸ\n",
        "#\n",
        "# ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š\n",
        "#   - {OUTPUT_DIR}/tw_monthly_revenue_{N}m.csv\n",
        "#   - åŒ…å«é—œéµ metadata æ¬„ä½ï¼šã€Œè³‡æ–™å¹´æœˆï¼ˆæ°‘åœ‹ï¼‰ã€ã€Œè³‡æ–™é¡å‹ï¼ˆå¦‚ sii@mopsov.twse.com.twï¼‰ã€\n",
        "#\n",
        "# âš ï¸ æ³¨æ„äº‹é …ï¼š\n",
        "#   - MOPS ç¶²ç«™å¶æœ‰ç¶­è­·æˆ–çµæ§‹å¾®èª¿ï¼Œè‹¥å¤§è¦æ¨¡å¤±æ•—è«‹æª¢æŸ¥ URL æ¨¡å¼æˆ– User-Agent\n",
        "#   - ã€Œå»å¹´åŒæœˆå¢æ¸›(%)ã€ç­‰è¨ˆç®—æ¬„ä½ç”±åŸå§‹ HTML æä¾›ï¼Œéæœ¬è…³æœ¬è¨ˆç®—ï¼ˆä¿ç•™åŸå§‹èªæ„ï¼‰\n",
        "#   - è‹¥éœ€ã€Œç´¯è¨ˆç‡Ÿæ”¶ YoYã€ç­‰è¡ç”ŸæŒ‡æ¨™ï¼Œå»ºè­°å¾ŒçºŒåœ¨ Pandas ä¸­å¦è¡Œè¨ˆç®—\n",
        "#   - è³‡æ–™å¹´æœˆæ ¼å¼ç‚ºã€Œæ°‘åœ‹_æœˆã€ï¼ˆå¦‚ 113_01ï¼‰ï¼Œç¬¦åˆå°ç£è²¡å ±æ…£ä¾‹\n",
        "#\n",
        "# ğŸ§ª é©—è­‰å»ºè­°ï¼š\n",
        "#   - æª¢æŸ¥è¼¸å‡ºçµ±è¨ˆä¸­çš„ã€Œè¦†è“‹å¹´æœˆç¯„åœã€æ˜¯å¦ç¬¦åˆé æœŸ\n",
        "#   - æŠ½æŸ¥å€‹è‚¡ï¼ˆå¦‚ 2330ã€3711ï¼‰ç¢ºèªç‡Ÿæ”¶æ•¸å€¼èˆ‡ MOPS ç¶²é ä¸€è‡´\n",
        "#   - è§€å¯Ÿã€Œpages_missã€æ˜¯å¦éé«˜ï¼ˆ>10% å¯èƒ½éœ€èª¿æ•´é‡è©¦ç­–ç•¥ï¼‰\n",
        "#\n",
        "# ğŸ“Œ ä½œè€…ï¼šData Engineering Team | æœ€å¾Œæ›´æ–°ï¼š2025 å¹´\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "# ğŸ“Œ [Cell 0] æŠ“å–æœ€è¿‘ N å€‹æœˆçš„æœˆç‡Ÿæ”¶ï¼ˆå«èˆŠå¹´å›æº¯ã€é›™ä¸»æ©Ÿ fallbackï¼‰â†’ CSV\n",
        "import os, time, requests, re\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from datetime import date, datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ================== âœ… å¯èª¿æ•´åƒæ•¸ ==================\n",
        "MONTHS_TO_FETCH   = 80     # 2025-09 å¾€å› 75 å€‹æœˆ â‰ˆ åˆ° 2019-...ï¼ˆæ¶µè“‹ 2020 èµ·ï¼‰\n",
        "END_GREG_YM       = \"2025-12\"  # è‹¥è¦ç²¾æº–åˆ° 2025-09 æ”¶æ–‚ï¼Œè¨­å®šé€™å€‹\n",
        "MAX_WORKERS       = 6\n",
        "OUTPUT_DIR        = \"/content\"\n",
        "FIXED_OUTPUT_NAME = f\"tw_monthly_revenue_{MONTHS_TO_FETCH}m.csv\"\n",
        "WATCHLIST_NAME    = f\"tw_monthly_revenue_{MONTHS_TO_FETCH}m_watchlist.csv\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "# ===================================================\n",
        "\n",
        "print(f\"ğŸš€ é–‹å§‹æŠ“å–æœ€è¿‘ {MONTHS_TO_FETCH} å€‹æœˆçš„æœˆç‡Ÿæ”¶è³‡æ–™...\")\n",
        "\n",
        "def greg_to_roc_year_month(ym: str):\n",
        "    # \"YYYY-MM\" -> (roc_year, month)\n",
        "    dt = datetime.strptime(ym, \"%Y-%m\")\n",
        "    return dt.year - 1911, dt.month\n",
        "\n",
        "def get_previous_months_roc_by_end(end_ym=END_GREG_YM, n=MONTHS_TO_FETCH):\n",
        "    \"\"\"ä¾æŒ‡å®šçµæŸå¹´æœˆï¼ˆè¥¿å…ƒ YYYY-MMï¼‰å¾€å› n å€‹æœˆï¼Œè¼¸å‡º ROC å¹´æœˆ tuple åˆ—è¡¨ã€‚\"\"\"\n",
        "    dt = datetime.strptime(end_ym, \"%Y-%m\")\n",
        "    out = []\n",
        "    y, m = dt.year, dt.month\n",
        "    for i in range(n):\n",
        "        yy = y\n",
        "        mm = m - i\n",
        "        while mm <= 0:\n",
        "            yy -= 1\n",
        "            mm += 12\n",
        "        out.append((yy - 1911, mm))\n",
        "    return out\n",
        "\n",
        "def fetch_table_with_retry(url, max_retries=4, delay=1.3):\n",
        "    \"\"\"å˜—è©¦æŠ“å– + è§£æ tableï¼›å¤±æ•—é‡è©¦ä¸¦æŒ‡æ•¸é€€é¿ã€‚\"\"\"\n",
        "    last_err = None\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = requests.get(url, headers={'User-Agent':'Mozilla/5.0'}, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            # èˆŠé å¤šç‚º big5ï¼Œä¹Ÿæœ‰ UTF-8ï¼Œè©¦ big5 å¤±æ•—å†ç”¨åŸå§‹\n",
        "            r.encoding = 'big5'\n",
        "            txt = r.text or \"\"\n",
        "            if len(txt) < 80 or '<table' not in txt.lower():\n",
        "                # å†è©¦ä¸æŒ‡å®šç·¨ç¢¼\n",
        "                r2 = requests.get(url, headers={'User-Agent':'Mozilla/5.0'}, timeout=30)\n",
        "                txt2 = r2.text or \"\"\n",
        "                if len(txt2) < 80 or '<table' not in txt2.lower():\n",
        "                    raise ValueError(\"å…§å®¹éçŸ­æˆ–ç„¡ <table>\")\n",
        "                tables = pd.read_html(StringIO(txt2))\n",
        "            else:\n",
        "                tables = pd.read_html(StringIO(txt))\n",
        "            if tables:\n",
        "                return tables\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if attempt == max_retries - 1:\n",
        "                return None\n",
        "            time.sleep(delay)\n",
        "            delay *= 1.6\n",
        "    return None\n",
        "\n",
        "def normalize_cols(df):\n",
        "    cols = []\n",
        "    for c in df.columns:\n",
        "        c = str(c).replace('\\u3000',' ').replace('ã€€',' ').replace('\\xa0',' ').strip()\n",
        "        c = re.sub(r'\\s+', ' ', c)\n",
        "        cols.append(c)\n",
        "    df.columns = cols\n",
        "    return df\n",
        "\n",
        "# æ¬„åæ˜ å°„ï¼ˆæ”¾å¯¬ï¼‰\n",
        "RENAME_MAP_BASE = {\n",
        "    'å…¬å¸ ä»£è™Ÿ':'å…¬å¸ä»£è™Ÿ',\n",
        "    'å…¬å¸ä»£è™Ÿ':'å…¬å¸ä»£è™Ÿ',\n",
        "    'è­‰åˆ¸ä»£è™Ÿ':'å…¬å¸ä»£è™Ÿ',\n",
        "    'è‚¡ç¥¨ä»£è™Ÿ':'å…¬å¸ä»£è™Ÿ',\n",
        "    'å…¬å¸åç¨±':'å…¬å¸åç¨±',\n",
        "    'å…¬å¸ç°¡ç¨±':'å…¬å¸åç¨±',\n",
        "    'åç¨±':'å…¬å¸åç¨±',\n",
        "    'ç•¶æœˆç‡Ÿæ”¶':'ç•¶æœˆç‡Ÿæ”¶',\n",
        "    'æœ¬æœˆç‡Ÿæ”¶':'ç•¶æœˆç‡Ÿæ”¶',\n",
        "    'ç•¶æœˆç‡Ÿæ¥­æ”¶å…¥':'ç•¶æœˆç‡Ÿæ”¶',\n",
        "    'ä¸Šæœˆç‡Ÿæ”¶':'ä¸Šæœˆç‡Ÿæ”¶',\n",
        "    'å»å¹´åŒæœˆç‡Ÿæ”¶':'å»å¹´åŒæœˆç‡Ÿæ”¶',\n",
        "    'å»å¹´ç•¶æœˆç‡Ÿæ”¶':'å»å¹´åŒæœˆç‡Ÿæ”¶',     # çµ±ä¸€åˆ°ã€Œå»å¹´åŒæœˆç‡Ÿæ”¶ã€\n",
        "    'å»å¹´åŒæœŸç‡Ÿæ”¶':'å»å¹´åŒæœˆç‡Ÿæ”¶',\n",
        "    'å»å¹´åŒæœˆç‡Ÿæ¥­æ”¶å…¥':'å»å¹´åŒæœˆç‡Ÿæ”¶',\n",
        "    'ä¸Šæœˆæ¯”è¼ƒ å¢æ¸›(%)':'ä¸Šæœˆæ¯”è¼ƒ å¢æ¸›(%)',\n",
        "    'å‰æœŸæ¯”è¼ƒ å¢æ¸›(%)':'å‰æœŸæ¯”è¼ƒ å¢æ¸›(%)',\n",
        "    'å‚™è¨»':'å‚™è¨»',\n",
        "}\n",
        "\n",
        "# å‹•æ…‹æŠŠã€Œå»å¹´ åŒæœˆ å¢æ¸›(%)ã€é€™ç¨®ç©ºæ ¼å·®ç•°ä¹Ÿä½µåˆ°åŒå\n",
        "def apply_rename(df):\n",
        "    df = normalize_cols(df)\n",
        "    ren = {}\n",
        "    for c in df.columns:\n",
        "        # æ•æ‰ã€Œå»å¹´â€¦åŒæœˆâ€¦å¢æ¸›(%)ã€\n",
        "        if re.match(r'^å»å¹´.*åŒæœˆ.*å¢æ¸›\\(%\\)\\s*$', c):\n",
        "            ren[c] = 'å»å¹´åŒæœˆ å¢æ¸›(%)'\n",
        "        elif c in RENAME_MAP_BASE:\n",
        "            ren[c] = RENAME_MAP_BASE[c]\n",
        "    return df.rename(columns=ren)\n",
        "\n",
        "def to_num(x):\n",
        "    if pd.isna(x): return pd.NA\n",
        "    x = str(x).replace(',','').replace('ï¼','-').replace('--','').strip()\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return pd.NA\n",
        "\n",
        "def accept_df(df):\n",
        "    \"\"\"é¬†ç¶é©—æ”¶ï¼šè¦æœ‰å…¬å¸ä»£è™Ÿ/å…¬å¸åç¨±/ç•¶æœˆç‡Ÿæ”¶ï¼ˆè‡³å°‘å…©è€…+ç•¶æœˆç‡Ÿæ”¶ï¼‰\"\"\"\n",
        "    need = set(df.columns)\n",
        "    ok = (('å…¬å¸ä»£è™Ÿ' in need or 'å…¬å¸åç¨±' in need) and ('ç•¶æœˆç‡Ÿæ”¶' in need))\n",
        "    return ok\n",
        "\n",
        "def build_urls(roc_y, m):\n",
        "    \"\"\"åŒä¸€å€‹ ROC å¹´æœˆï¼Œå…©çµ„ä¸»æ©Ÿè¼ªæµå˜—è©¦ã€‚\"\"\"\n",
        "    paths = [\n",
        "        ('sii', 't21/sii/t21sc03_{y}_{m}_0.html'),\n",
        "        ('otc', 't21/otc/t21sc03_{y}_{m}_0.html'),\n",
        "        ('rotc','t21/rotc/t21sc03_{y}_{m}_0.html'),\n",
        "        ('sii_ky','t21/sii/t21sc03_{y}_{m}_1.html'),\n",
        "        ('otc_ky','t21/otc/t21sc03_{y}_{m}_1.html'),\n",
        "        ('rotc_ky','t21/rotc/t21sc03_{y}_{m}_1.html'),\n",
        "    ]\n",
        "    hosts = [\n",
        "        'mopsov.twse.com.tw',  # è¿‘å¹´\n",
        "        'mops.twse.com.tw',    # èˆŠå¹´\n",
        "    ]\n",
        "    urls = []\n",
        "    for h in hosts:\n",
        "        for tag, pat in paths:\n",
        "            urls.append((f\"{tag}@{h}\", f\"https://{h}/nas/{pat.format(y=roc_y, m=m)}\"))\n",
        "    return urls\n",
        "\n",
        "def fetch_single_month(roc_y, m):\n",
        "    yymm = f\"{roc_y}_{int(m):02d}\"\n",
        "    # å˜—è©¦é›™ä¸»æ©Ÿ + å…­é¡é é¢\n",
        "    processed = []\n",
        "    for key, url in build_urls(roc_y, m):\n",
        "        tables = fetch_table_with_retry(url)\n",
        "        if not tables:\n",
        "            continue\n",
        "        for t in tables:\n",
        "            # æ‰å¹³æ¬„å\n",
        "            if isinstance(t.columns, pd.MultiIndex):\n",
        "                t.columns = t.columns.get_level_values(-1)\n",
        "            t = apply_rename(t)\n",
        "            if not accept_df(t):\n",
        "                continue\n",
        "            # æ¸…ç†åˆè¨ˆåˆ—\n",
        "            if 'å…¬å¸åç¨±' in t.columns:\n",
        "                t = t[~t['å…¬å¸åç¨±'].astype(str).str.contains('åˆè¨ˆ|å…¨éƒ¨', na=False)]\n",
        "            if 'å…¬å¸ä»£è™Ÿ' in t.columns:\n",
        "                t['å…¬å¸ä»£è™Ÿ'] = t['å…¬å¸ä»£è™Ÿ'].astype(str).str.strip()\n",
        "                t = t[t['å…¬å¸ä»£è™Ÿ'].str.contains(r'\\d', na=False)]\n",
        "\n",
        "            # æ•¸å€¼è½‰æ›\n",
        "            for c in ['ç•¶æœˆç‡Ÿæ”¶','ä¸Šæœˆç‡Ÿæ”¶','å»å¹´åŒæœˆç‡Ÿæ”¶','ä¸Šæœˆæ¯”è¼ƒ å¢æ¸›(%)','å»å¹´åŒæœˆ å¢æ¸›(%)',\n",
        "                      'ç•¶æœˆç´¯è¨ˆç‡Ÿæ”¶','å»å¹´ç´¯è¨ˆç‡Ÿæ”¶','å‰æœŸæ¯”è¼ƒ å¢æ¸›(%)']:\n",
        "                if c in t.columns:\n",
        "                    t[c] = t[c].map(to_num)\n",
        "\n",
        "            # è£œ metadata æ¬„\n",
        "            t['è³‡æ–™å¹´æœˆ'] = yymm\n",
        "            t['è³‡æ–™é¡å‹'] = key\n",
        "            processed.append(t)\n",
        "\n",
        "        # åªè¦æŸä¸»æ©Ÿ+é é¢æŠ“åˆ°æœ‰æ•ˆè¡¨æ ¼ï¼Œå°±ç¹¼çºŒå˜—è©¦å…¶ä»–é é¢ï¼›ä¸è¦éæ—© breakï¼Œå› ç‚ºå…­é¡è¦æ•´ä½µ\n",
        "    return processed\n",
        "\n",
        "def download_monthly_reports(roc_months, max_workers=MAX_WORKERS):\n",
        "    tasks = [(y, m) for (y, m) in roc_months]\n",
        "    all_tables, stats = [], {'months':len(tasks), 'pages_ok':0, 'pages_miss':0}\n",
        "\n",
        "    def _worker(y, m):\n",
        "        try:\n",
        "            return fetch_single_month(y, m)\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futs = {ex.submit(_worker, y, m): (y, m) for (y, m) in tasks}\n",
        "        with tqdm(total=len(futs), desc=\"ä¸‹è¼‰æœˆä»½\", unit=\"æœˆ\") as pbar:\n",
        "            for fut in as_completed(futs):\n",
        "                res = fut.result()\n",
        "                if res:\n",
        "                    all_tables.extend(res)\n",
        "                    stats['pages_ok'] += len(res)\n",
        "                else:\n",
        "                    stats['pages_miss'] += 1\n",
        "                pbar.set_postfix(stats)\n",
        "                pbar.update(1)\n",
        "    print(\"\\n=== ä¸‹è¼‰çµ±è¨ˆ ===\")\n",
        "    for k,v in stats.items(): print(f\"{k}: {v}\")\n",
        "    return all_tables\n",
        "\n",
        "# --- åŸ·è¡ŒæŠ“å– ---\n",
        "roc_months = get_previous_months_roc_by_end(END_GREG_YM, MONTHS_TO_FETCH)\n",
        "print(\"è™•ç†æœˆä»½ (æ°‘åœ‹å¹´, æœˆ):\", roc_months[:5], \"...\", roc_months[-5:])  # å‰å¾Œå„åˆ— 5 ç­†\n",
        "print(\"\\né–‹å§‹ä¸‹è¼‰â€¦\")\n",
        "tables = download_monthly_reports(roc_months)\n",
        "\n",
        "if not tables:\n",
        "    raise SystemExit(\"âŒ æŠ“å–å¤±æ•—ï¼šç„¡ä»»ä½•è¡¨æ ¼\")\n",
        "\n",
        "merged = pd.concat(tables, ignore_index=True)\n",
        "\n",
        "# æ¬„ä½æ’åºï¼šæŠŠè³‡æ–™å¹´æœˆ/è³‡æ–™é¡å‹æ”¾å‰é¢\n",
        "front = ['è³‡æ–™å¹´æœˆ','è³‡æ–™é¡å‹']\n",
        "cols = front + [c for c in merged.columns if c not in front]\n",
        "merged = merged[cols]\n",
        "\n",
        "# å»é‡ï¼ˆåŒæœˆåŒæ¿å¡ŠåŒä»£è™Ÿæ‹¿ç¬¬ä¸€ç­†ï¼‰\n",
        "keys = ['è³‡æ–™å¹´æœˆ','è³‡æ–™é¡å‹','å…¬å¸ä»£è™Ÿ']\n",
        "if all(k in merged.columns for k in keys):\n",
        "    before = len(merged)\n",
        "    merged = merged.drop_duplicates(subset=keys, keep='first')\n",
        "    print(f\"å»é‡ï¼š{before} â†’ {len(merged)}\")\n",
        "\n",
        "# è¼¸å‡º\n",
        "out_csv = os.path.join(OUTPUT_DIR, FIXED_OUTPUT_NAME)\n",
        "merged.to_csv(out_csv, index=False, encoding='utf_8_sig')\n",
        "print(f\"\\nâœ… æœˆç‡Ÿæ”¶è³‡æ–™å·²å„²å­˜ï¼š{out_csv}\")\n",
        "print(f\"ğŸ“Œ æª”åï¼š{FIXED_OUTPUT_NAME}\")\n",
        "\n",
        "# ===== è¦†è“‹ç¯„åœå¥æª¢ï¼ˆå‹™å¿…çœ‹é€™ä¸€æ®µçš„è¼¸å‡ºï¼‰=====\n",
        "def roc_to_greg(ym):\n",
        "    try:\n",
        "        y, m = str(ym).split('_')\n",
        "        return f\"{int(y)+1911}-{int(m):02d}\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "tmp = merged.copy()\n",
        "tmp['å¹´æœˆ'] = pd.to_datetime(tmp['è³‡æ–™å¹´æœˆ'].map(roc_to_greg), errors='coerce').dt.to_period('M')\n",
        "print(\"\\nğŸ§­ æŠ“å–è¦†è“‹ï¼š\", tmp['å¹´æœˆ'].min(), \"â†’\", tmp['å¹´æœˆ'].max())\n",
        "print(\"ğŸ—“ï¸ å¹´åº¦åˆ†ä½ˆï¼ˆç­†æ•¸ï¼‰ï¼š\")\n",
        "print(tmp['å¹´æœˆ'].dt.year.value_counts().sort_index())\n",
        "print(\"ğŸ” æ¬„åå¿«ç…§ï¼š\", list(merged.columns)[:16])\n"
      ],
      "metadata": {
        "id": "2g4b1sXbQhIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ğŸ—ƒï¸ [Cell 1] å°‡æœˆç‡Ÿæ”¶ CSV æª”æ¡ˆæ¨™æº–åŒ–ä¸¦åŒ¯å…¥æœ¬åœ° SQLite è³‡æ–™åº«\n",
        "# =============================================================================\n",
        "# ğŸ” åŠŸèƒ½èªªæ˜ï¼š\n",
        "#   - è®€å–ç”± MOPS æŠ“å–çš„åŸå§‹æœˆç‡Ÿæ”¶ CSVï¼ˆå¦‚ tw_monthly_revenue_80m.csvï¼‰\n",
        "#   - åŸ·è¡Œæ¬„ä½æ¨™æº–åŒ–ã€æ•¸å€¼æ¸…æ´—ã€é¡å‹è½‰æ›\n",
        "#   - åŒ¯å…¥è‡³è¼•é‡ç´š SQLite è³‡æ–™åº«ï¼ˆlocal_taiwan_stock.dbï¼‰\n",
        "#   - è‡ªå‹•å»ºç«‹é—œéµç´¢å¼•ä»¥åŠ é€Ÿå¾ŒçºŒåˆ†ææŸ¥è©¢\n",
        "#\n",
        "# âš™ï¸ æ ¸å¿ƒè™•ç†æ­¥é©Ÿï¼š\n",
        "#   âœ… æ¬„ä½æ˜ å°„ï¼šå°‡ä¸­æ–‡æ¬„åçµ±ä¸€è½‰ç‚ºè‹±æ–‡ snake_caseï¼ˆå¦‚ã€Œç•¶æœˆç‡Ÿæ”¶ã€â†’ rev_currentï¼‰\n",
        "#   âœ… æ•¸å€¼æ¸…æ´—ï¼šç§»é™¤åƒåˆ†ä½é€—è™Ÿã€è™•ç†ã€Œ--ã€ã€Œï¼ã€ç­‰ç„¡æ•ˆå€¼ï¼Œè½‰ç‚º float / NaN\n",
        "#   âœ… é‡è¤‡æ¬„ä½åˆä½µï¼šã€Œå»å¹´ç•¶æœˆç‡Ÿæ”¶ã€èˆ‡ã€Œå»å¹´åŒæœˆç‡Ÿæ”¶ã€çµ±ä¸€ç‚º rev_last_year\n",
        "#   âœ… å®‰å…¨å¯«å…¥ï¼šä½¿ç”¨ if_exists='replace' ç¢ºä¿æ¯æ¬¡éƒ½æ˜¯æœ€æ–°å®Œæ•´è³‡æ–™é›†\n",
        "#   âœ… æ•ˆèƒ½å„ªåŒ–ï¼šå»ºç«‹ä¸‰å€‹é—œéµç´¢å¼•ï¼š\n",
        "#        - idx_rev_id:      stock_idï¼ˆå¿«é€Ÿç¯©é¸å€‹è‚¡ï¼‰\n",
        "#        - idx_rev_date:    report_monthï¼ˆæŒ‰æœˆä»½æŸ¥è©¢ï¼‰\n",
        "#        - idx_yoy_high:    yoy_pctï¼ˆç”¨æ–¼é«˜æˆé•·è‚¡ç¯©é¸ï¼‰\n",
        "#\n",
        "# ğŸ“ è¼¸å…¥ï¼è¼¸å‡ºï¼š\n",
        "#   - è¼¸å…¥ï¼štw_monthly_revenue_80m.csvï¼ˆéœ€ä½æ–¼åŒç›®éŒ„ï¼‰\n",
        "#   - è¼¸å‡ºï¼šlocal_taiwan_stock.dbï¼ˆSQLite æª”æ¡ˆï¼Œå« monthly_revenue è¡¨ï¼‰\n",
        "#\n",
        "# âš ï¸ æ³¨æ„äº‹é …ï¼š\n",
        "#   - è‹¥åŸ·è¡Œæ™‚å ±éŒ¯ã€Œdatabase is lockedã€ï¼Œè«‹é—œé–‰æ‰€æœ‰å¯èƒ½ä½”ç”¨ DB çš„å·¥å…·ï¼ˆå¦‚ VS Code SQLite å¤–æ›ã€DBeaverã€å…¶ä»– Python è…³æœ¬ï¼‰\n",
        "#   - æ•¸å€¼æ¬„ä½è‹¥åŸå§‹ç‚º '--' æˆ–ç©ºç™½ï¼Œæœƒè¢«è½‰ç‚º NULLï¼ˆpd.NA â†’ SQLite NULLï¼‰\n",
        "#   - æ­¤è…³æœ¬ã€Œä¸ä¿ç•™æ­·å²ç‰ˆæœ¬ã€ï¼Œæ¯æ¬¡åŸ·è¡Œéƒ½æœƒè¦†è“‹æ•´å€‹ monthly_revenue è¡¨\n",
        "#   - report_month æ ¼å¼ç¶­æŒæ°‘åœ‹å¹´æœˆï¼ˆå¦‚ \"113_01\"ï¼‰ï¼Œèˆ‡ä¸Šæ¸¸æŠ“å–ä¸€è‡´ï¼Œä¾¿æ–¼ä¸²æ¥\n",
        "#\n",
        "# ğŸ’¡ å¾ŒçºŒå»ºè­°ï¼š\n",
        "#   - å¯æ­é… stock_annual_k / stock_monthly_k è¡¨é€²è¡Œã€Œç‡Ÿæ”¶ vs è‚¡åƒ¹ã€äº¤å‰åˆ†æ\n",
        "#   - è‹¥éœ€æ”¯æ´è¥¿å…ƒå¹´æœˆæŸ¥è©¢ï¼Œå¯æ–°å¢è¨ˆç®—æ¬„ä½æˆ–å»ºç«‹ VIEW\n",
        "#\n",
        "# ğŸ“Œ ä½œè€…ï¼šData Pipeline Team | æœ€å¾Œæ›´æ–°ï¼š2025 å¹´\n",
        "# =============================================================================\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. è¨­å®šè·¯å¾‘\n",
        "csv_file = 'tw_monthly_revenue_80m.csv'\n",
        "db_file = 'local_taiwan_stock.db'\n",
        "\n",
        "if not os.path.exists(csv_file):\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_file}\")\n",
        "    exit()\n",
        "\n",
        "print(\"ğŸ“– æ­£åœ¨è®€å– CSV æª”æ¡ˆ...\")\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# 2. å®šç¾©æ¬„ä½æ˜ å°„\n",
        "column_mapping = {\n",
        "    'è³‡æ–™å¹´æœˆ': 'report_month',\n",
        "    'è³‡æ–™é¡å‹': 'market_type',\n",
        "    'å…¬å¸ä»£è™Ÿ': 'stock_id',\n",
        "    'å…¬å¸åç¨±': 'stock_name',\n",
        "    'ç•¶æœˆç‡Ÿæ”¶': 'rev_current',\n",
        "    'ä¸Šæœˆç‡Ÿæ”¶': 'rev_last_month',\n",
        "    'å»å¹´ç•¶æœˆç‡Ÿæ”¶': 'rev_last_year',\n",
        "    'å»å¹´åŒæœˆç‡Ÿæ”¶': 'rev_last_year',\n",
        "    'ä¸Šæœˆæ¯”è¼ƒ å¢æ¸›(%)': 'mom_pct',\n",
        "    'å»å¹´åŒæœˆ å¢æ¸›(%)': 'yoy_pct',\n",
        "    'ç•¶æœˆç´¯è¨ˆç‡Ÿæ”¶': 'rev_accumulated',\n",
        "    'å»å¹´ç´¯è¨ˆç‡Ÿæ”¶': 'rev_accumulated_last_year',\n",
        "    'å‰æœŸæ¯”è¼ƒ å¢æ¸›(%)': 'yoy_accumulated_pct',\n",
        "    'å‚™è¨»': 'remark'\n",
        "}\n",
        "\n",
        "print(\"ğŸ§¹ æ­£åœ¨æ¨™æº–åŒ–æ¬„ä½èˆ‡è³‡æ–™æ¸…ç†...\")\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "df = df.rename(columns=column_mapping)\n",
        "\n",
        "# æ•¸å€¼æ¸…ç†\n",
        "numeric_cols = ['rev_current', 'rev_last_month', 'rev_last_year', 'mom_pct', 'yoy_pct']\n",
        "for col in numeric_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '').replace('--', '').replace('ï¼', ''), errors='coerce')\n",
        "\n",
        "# 3. å¯«å…¥è³‡æ–™åº« (å¸¶é€²åº¦æ¢)\n",
        "try:\n",
        "    print(f\"ğŸ“¦ æ­£åœ¨å¯«å…¥è³‡æ–™åº«: {db_file} ...\")\n",
        "    conn = sqlite3.connect(db_file, timeout=30) # å¢åŠ  timeout é˜²æ­¢é–å®š\n",
        "\n",
        "    # å­˜å…¥è³‡æ–™åº«\n",
        "    df.to_sql('monthly_revenue', conn, if_exists='replace', index=False)\n",
        "\n",
        "    print(\"âš¡ æ­£åœ¨å»ºç«‹ç´¢å¼• (Index) ä»¥å„ªåŒ–æŸ¥è©¢é€Ÿåº¦...\")\n",
        "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_rev_id ON monthly_revenue (stock_id)\")\n",
        "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_rev_date ON monthly_revenue (report_month)\")\n",
        "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_yoy_high ON monthly_revenue (yoy_pct)\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"âœ… æˆåŠŸï¼å·²åŒ¯å…¥ {len(df)} ç­†æ•¸æ“šã€‚\")\n",
        "\n",
        "except sqlite3.OperationalError as e:\n",
        "    print(f\"âŒ éŒ¯èª¤ï¼šè³‡æ–™åº«å¯èƒ½è¢«å…¶ä»–ç¨‹å¼ä½”ç”¨ä¸­ã€‚è«‹é—œé–‰ VS Code çš„ SQLite ç€è¦½å™¨æˆ–å…¶å®ƒ Python ç¨‹å¼ã€‚\")\n",
        "    print(f\"è©³ç´°éŒ¯èª¤è¨Šæ¯: {e}\")"
      ],
      "metadata": {
        "id": "oabG6l4rZqs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ğŸ” [Cell 2] SQLite è³‡æ–™åº«è¨ºæ–·å·¥å…·ï¼šæª¢è¦–è¡¨çµæ§‹ + éš¨æ©ŸæŠ½æ¨£è³‡æ–™\n",
        "# =============================================================================\n",
        "# ğŸ” åŠŸèƒ½èªªæ˜ï¼š\n",
        "#   - æª¢æŸ¥æœ¬åœ° SQLite è³‡æ–™åº«ï¼ˆlocal_taiwan_stock.dbï¼‰æ˜¯å¦å­˜åœ¨\n",
        "#   - åˆ—å‡ºæ‰€æœ‰è³‡æ–™è¡¨ï¼ˆtableï¼‰\n",
        "#   - å°æ¯å¼µè¡¨é¡¯ç¤ºï¼š\n",
        "#       â€¢ æ¬„ä½åç¨±èˆ‡ SQLite å‹åˆ¥ï¼ˆTEXT / REAL / INTEGER ç­‰ï¼‰\n",
        "#       â€¢ ç¸½ç­†æ•¸ï¼ˆæ ¼å¼åŒ–åƒåˆ†ä½ï¼‰\n",
        "#       â€¢ éš¨æ©ŸæŠ½å– 1ï½3 ç­†å¯¦éš›è³‡æ–™ï¼ˆç”¨æ–¼å¿«é€Ÿé©—è­‰å…§å®¹æ­£ç¢ºæ€§ï¼‰\n",
        "#\n",
        "# âš™ï¸ è¨­è¨ˆäº®é»ï¼š\n",
        "#   âœ… å®‰å…¨è®€å–ï¼šä½¿ç”¨ pandas + PRAGMA é¿å…æ‰‹å‹•è§£æ schema\n",
        "#   âœ… é˜²å‘†æ©Ÿåˆ¶ï¼šè™•ç†ç©ºè¡¨ã€æå£ DBã€è®€å–ç•°å¸¸ç­‰é‚Šç•Œæƒ…æ³\n",
        "#   âœ… ç›´è§€è¼¸å‡ºï¼šè¡¨æ ¼åŒ–å‘ˆç¾ï¼Œä¾¿æ–¼äººå·¥æ ¸å°ï¼ˆå¦‚ç¢ºèª rev_current æ˜¯å¦ç‚ºæ•¸å€¼ï¼‰\n",
        "#   âœ… éç ´å£æ€§ï¼šåƒ…è®€å–ï¼Œä¸ä¿®æ”¹ä»»ä½•è³‡æ–™\n",
        "#\n",
        "# ğŸ“ è¼¸å…¥ï¼š\n",
        "#   - local_taiwan_stock.dbï¼ˆé æœŸåŒ…å« monthly_revenueã€stock_monthly_k ç­‰è¡¨ï¼‰\n",
        "#\n",
        "# ğŸ¯ ä½¿ç”¨æƒ…å¢ƒï¼š\n",
        "#   - é©—è­‰å‰ä¸€æ­¥é©Ÿï¼ˆCSV åŒ¯å…¥ï¼‰æ˜¯å¦æˆåŠŸ\n",
        "#   - å¿«é€Ÿç¢ºèªæ¬„ä½å‘½åèˆ‡è³‡æ–™é¡å‹æ˜¯å¦ç¬¦åˆåˆ†æéœ€æ±‚\n",
        "#   - æ’æŸ¥ã€Œç‚ºä½• JOIN æŸ¥ä¸åˆ°è³‡æ–™ï¼Ÿã€ç­‰å•é¡Œ\n",
        "#   - åˆ†äº«è³‡æ–™åº«çµ¦åŒäº‹æ™‚ï¼Œæä¾›è‡ªåŠ©å¼æª¢è¦–æ–¹å¼\n",
        "#\n",
        "# âš ï¸ æ³¨æ„äº‹é …ï¼š\n",
        "#   - è‹¥å ±éŒ¯ \"database disk image is malformed\"ï¼Œè¡¨ç¤º DB æª”æ¡ˆæå£ï¼Œéœ€é‡æ–°ç”Ÿæˆ\n",
        "#   - éš¨æ©ŸæŠ½æ¨£ä½¿ç”¨ ORDER BY RANDOM()ï¼Œåœ¨å¤§è¡¨ä¸Šå¯èƒ½ç¨æ…¢ï¼ˆä½†æœ¬è…³æœ¬åƒ…é™å°è¦æ¨¡æœ¬åœ° DBï¼‰\n",
        "#   - æ¬„ä½å‹åˆ¥é¡¯ç¤ºçš„æ˜¯ SQLite å¯¦éš›å„²å­˜å‹æ…‹ï¼ˆé Pandas dtypeï¼‰ï¼Œå¯èƒ½ç‚º \"\"ï¼ˆæœªæŒ‡å®šï¼‰\n",
        "#\n",
        "# ğŸ’¡ å»ºè­°æ­é…ï¼š\n",
        "#   - åŸ·è¡Œæ–¼ CSV åŒ¯å…¥å¾Œã€æ­£å¼åˆ†æå‰\n",
        "#   - èˆ‡ã€Œæœˆç‡Ÿæ”¶æŠ“å–ã€å’Œã€Œè‚¡åƒ¹èšåˆã€æµç¨‹ä¸²æˆå®Œæ•´ pipeline\n",
        "#\n",
        "# ğŸ“Œ ä½œè€…ï¼šData Validation Team | æœ€å¾Œæ›´æ–°ï¼š2025 å¹´\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "db_path = \"local_taiwan_stock.db\"\n",
        "\n",
        "# æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
        "if not os.path.exists(db_path):\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ°è³‡æ–™åº«æª”æ¡ˆ: {db_path}\")\n",
        "else:\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # 1. åˆ—å‡ºæ‰€æœ‰è³‡æ–™è¡¨\n",
        "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "        tables = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "        if not tables:\n",
        "            print(\"ğŸ“­ è³‡æ–™åº«ä¸­æ²’æœ‰ä»»ä½•è³‡æ–™è¡¨ã€‚\")\n",
        "        else:\n",
        "            print(f\"ğŸ“‹ è³‡æ–™åº« '{db_path}' ä¸­çš„è³‡æ–™è¡¨ï¼š\")\n",
        "            for table in tables:\n",
        "                print(f\"\\n--- è³‡æ–™è¡¨: {table} ---\")\n",
        "\n",
        "                # 2. å–å¾—æ¬„ä½è³‡è¨Š\n",
        "                cursor.execute(f\"PRAGMA table_info({table});\")\n",
        "                columns = cursor.fetchall()\n",
        "                col_names = [col[1] for col in columns]\n",
        "                col_types = [col[2] for col in columns]\n",
        "                print(f\"æ¬„ä½åç¨±: {col_names}\")\n",
        "                print(f\"æ¬„ä½é¡å‹: {col_types}\")\n",
        "\n",
        "                # 3. éš¨æ©ŸæŠ½æ¨£å¹¾ç­†è³‡æ–™ï¼ˆç”¨ pandas æ›´æ–¹ä¾¿ï¼‰\n",
        "                try:\n",
        "                    # å…ˆçœ‹ç¸½ç­†æ•¸\n",
        "                    total = pd.read_sql(f\"SELECT COUNT(*) AS cnt FROM {table}\", conn).iloc[0]['cnt']\n",
        "                    print(f\"ç¸½ç­†æ•¸: {total:,}\")\n",
        "\n",
        "                    if total > 0:\n",
        "                        # éš¨æ©Ÿå– 3 ç­†ï¼ˆè‹¥è³‡æ–™å°‘æ–¼ 3 ç­†å‰‡å…¨å–ï¼‰\n",
        "                        sample_size = min(3, total)\n",
        "                        # ä½¿ç”¨ RANDOM() æŠ½æ¨£\n",
        "                        df_sample = pd.read_sql(\n",
        "                            f\"SELECT * FROM {table} ORDER BY RANDOM() LIMIT {sample_size}\",\n",
        "                            conn\n",
        "                        )\n",
        "                        print(\"éš¨æ©Ÿæ¨£æœ¬:\")\n",
        "                        print(df_sample.to_string(index=False))\n",
        "                    else:\n",
        "                        print(\"ï¼ˆç„¡è³‡æ–™ï¼‰\")\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ è®€å–è³‡æ–™å¤±æ•—: {e}\")\n",
        "\n",
        "        conn.close()\n",
        "        print(\"\\nâœ… è³‡æ–™åº«æª¢æŸ¥å®Œæˆã€‚\")\n",
        "\n",
        "    except sqlite3.DatabaseError as e:\n",
        "        print(f\"âŒ SQLite è³‡æ–™åº«æå£: {e}\")\n",
        "        print(\"è«‹é‡æ–°ä¸Šå‚³å®Œå¥½çš„ .db æª”æ¡ˆï¼\")"
      ],
      "metadata": {
        "id": "zlFNp_51aro2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ğŸ” [Cell 2] SQLite è³‡æ–™åº«è¨ºæ–·å·¥å…·ï¼šæª¢è¦–è¡¨çµæ§‹ + éš¨æ©ŸæŠ½æ¨£è³‡æ–™\n",
        "# =============================================================================\n",
        "# ğŸ” åŠŸèƒ½èªªæ˜ï¼š\n",
        "#   - æª¢æŸ¥æœ¬åœ° SQLite è³‡æ–™åº«ï¼ˆlocal_taiwan_stock.dbï¼‰æ˜¯å¦å­˜åœ¨\n",
        "#   - åˆ—å‡ºæ‰€æœ‰è³‡æ–™è¡¨ï¼ˆtableï¼‰\n",
        "#   - å°æ¯å¼µè¡¨é¡¯ç¤ºï¼š\n",
        "#       â€¢ æ¬„ä½åç¨±èˆ‡ SQLite å‹åˆ¥ï¼ˆTEXT / REAL / INTEGER ç­‰ï¼‰\n",
        "#       â€¢ ç¸½ç­†æ•¸ï¼ˆæ ¼å¼åŒ–åƒåˆ†ä½ï¼‰\n",
        "#       â€¢ éš¨æ©ŸæŠ½å– 1ï½3 ç­†å¯¦éš›è³‡æ–™ï¼ˆç”¨æ–¼å¿«é€Ÿé©—è­‰å…§å®¹æ­£ç¢ºæ€§ï¼‰\n",
        "#\n",
        "# âš™ï¸ è¨­è¨ˆäº®é»ï¼š\n",
        "#   âœ… å®‰å…¨è®€å–ï¼šä½¿ç”¨ pandas + PRAGMA é¿å…æ‰‹å‹•è§£æ schema\n",
        "#   âœ… é˜²å‘†æ©Ÿåˆ¶ï¼šè™•ç†ç©ºè¡¨ã€æå£ DBã€è®€å–ç•°å¸¸ç­‰é‚Šç•Œæƒ…æ³\n",
        "#   âœ… ç›´è§€è¼¸å‡ºï¼šè¡¨æ ¼åŒ–å‘ˆç¾ï¼Œä¾¿æ–¼äººå·¥æ ¸å°ï¼ˆå¦‚ç¢ºèª rev_current æ˜¯å¦ç‚ºæ•¸å€¼ï¼‰\n",
        "#   âœ… éç ´å£æ€§ï¼šåƒ…è®€å–ï¼Œä¸ä¿®æ”¹ä»»ä½•è³‡æ–™\n",
        "#\n",
        "# ğŸ“ è¼¸å…¥ï¼š\n",
        "#   - local_taiwan_stock.dbï¼ˆé æœŸåŒ…å« monthly_revenueã€stock_monthly_k ç­‰è¡¨ï¼‰\n",
        "#\n",
        "# ğŸ¯ ä½¿ç”¨æƒ…å¢ƒï¼š\n",
        "#   - é©—è­‰å‰ä¸€æ­¥é©Ÿï¼ˆCSV åŒ¯å…¥ï¼‰æ˜¯å¦æˆåŠŸ\n",
        "#   - å¿«é€Ÿç¢ºèªæ¬„ä½å‘½åèˆ‡è³‡æ–™é¡å‹æ˜¯å¦ç¬¦åˆåˆ†æéœ€æ±‚\n",
        "#   - æ’æŸ¥ã€Œç‚ºä½• JOIN æŸ¥ä¸åˆ°è³‡æ–™ï¼Ÿã€ç­‰å•é¡Œ\n",
        "#   - åˆ†äº«è³‡æ–™åº«çµ¦åŒäº‹æ™‚ï¼Œæä¾›è‡ªåŠ©å¼æª¢è¦–æ–¹å¼\n",
        "#\n",
        "# âš ï¸ æ³¨æ„äº‹é …ï¼š\n",
        "#   - è‹¥å ±éŒ¯ \"database disk image is malformed\"ï¼Œè¡¨ç¤º DB æª”æ¡ˆæå£ï¼Œéœ€é‡æ–°ç”Ÿæˆ\n",
        "#   - éš¨æ©ŸæŠ½æ¨£ä½¿ç”¨ ORDER BY RANDOM()ï¼Œåœ¨å¤§è¡¨ä¸Šå¯èƒ½ç¨æ…¢ï¼ˆä½†æœ¬è…³æœ¬åƒ…é™å°è¦æ¨¡æœ¬åœ° DBï¼‰\n",
        "#   - æ¬„ä½å‹åˆ¥é¡¯ç¤ºçš„æ˜¯ SQLite å¯¦éš›å„²å­˜å‹æ…‹ï¼ˆé Pandas dtypeï¼‰ï¼Œå¯èƒ½ç‚º \"\"ï¼ˆæœªæŒ‡å®šï¼‰\n",
        "#\n",
        "# ğŸ’¡ å»ºè­°æ­é…ï¼š\n",
        "#   - åŸ·è¡Œæ–¼ CSV åŒ¯å…¥å¾Œã€æ­£å¼åˆ†æå‰\n",
        "#   - èˆ‡ã€Œæœˆç‡Ÿæ”¶æŠ“å–ã€å’Œã€Œè‚¡åƒ¹èšåˆã€æµç¨‹ä¸²æˆå®Œæ•´ pipeline\n",
        "#\n",
        "# ğŸ“Œ ä½œè€…ï¼šData Validation Team | æœ€å¾Œæ›´æ–°ï¼š2025 å¹´\n",
        "# =============================================================================\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "\n",
        "# --- é…ç½®å€ ---\n",
        "DB_PASSWORD = \"Supabase Service Role å¯†ç¢¼\"          # ä½ çš„ Supabase Service Role å¯†ç¢¼\n",
        "PROJECT_REF = \"å°ˆæ¡ˆ IDï¼ˆpostgres.<PROJECT_REF>ï¼‰\"     # å°ˆæ¡ˆ IDï¼ˆpostgres.<PROJECT_REF>ï¼‰\n",
        "POOLER_HOST = \"aws-1-ap-southeast-1.pooler.supabase.com\"\n",
        "connection_string = f\"postgresql://postgres.{PROJECT_REF}:{DB_PASSWORD}@{POOLER_HOST}:5432/postgres\"\n",
        "\n",
        "print(\"ğŸ”„ é–‹å§‹åŸ·è¡Œæ™ºæ…§åŒæ­¥ï¼šåªä¸Šå‚³æ–°å¢çš„è‚¡åƒ¹è³‡æ–™\")\n",
        "\n",
        "# 1. å¾æœ¬åœ° SQLite å–å¾—æœ€æ–°äº¤æ˜“æ—¥\n",
        "print(\"ğŸ” æ­¥é©Ÿ 1/3ï¼šè®€å–æœ¬åœ°æœ€æ–°äº¤æ˜“æ—¥æœŸ...\")\n",
        "conn_sqlite = sqlite3.connect(\"local_taiwan_stock.db\")\n",
        "local_max_date = pd.read_sql(\"SELECT MAX(date) AS max_date FROM stock_prices\", conn_sqlite).iloc[0]['max_date']\n",
        "conn_sqlite.close()\n",
        "print(f\"ğŸ“… æœ¬åœ°æœ€æ–°äº¤æ˜“æ—¥: {local_max_date}\")\n",
        "\n",
        "# 2. é€£æ¥ Supabaseï¼Œå–å¾—é ç«¯æœ€æ–°äº¤æ˜“æ—¥\n",
        "engine = create_engine(connection_string)\n",
        "remote_max_date = None\n",
        "\n",
        "try:\n",
        "    print(\"ğŸ” æ­¥é©Ÿ 2/3ï¼šæŸ¥è©¢ Supabase æœ€æ–°äº¤æ˜“æ—¥æœŸ...\")\n",
        "    with engine.connect() as conn:\n",
        "        try:\n",
        "            result = conn.execute(text(\"SELECT MAX(date) AS max_date FROM stock_prices\"))\n",
        "            row = result.fetchone()\n",
        "            remote_max_date = row[0] if row and row[0] else None\n",
        "            print(f\"â˜ï¸ Supabase æœ€æ–°äº¤æ˜“æ—¥: {remote_max_date or 'ï¼ˆç„¡è³‡æ–™ï¼‰'}\")\n",
        "        except Exception as e:\n",
        "            if 'relation \"stock_prices\" does not exist' in str(e):\n",
        "                print(\"ğŸ†• Supabase å°šæœªå»ºç«‹ stock_prices è¡¨ï¼ˆé¦–æ¬¡åŒæ­¥ï¼‰\")\n",
        "                remote_max_date = None\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    # 3. æ±ºå®šè¦ä¸Šå‚³å“ªäº›è³‡æ–™\n",
        "    if remote_max_date is None:\n",
        "        print(\"ğŸ“¤ åˆ¤æ–·ï¼šé¦–æ¬¡åŒæ­¥ â†’ ä¸Šå‚³å…¨éƒ¨æœ¬åœ°è³‡æ–™\")\n",
        "        conn_sqlite = sqlite3.connect(\"local_taiwan_stock.db\")\n",
        "        upload_df = pd.read_sql(\"SELECT * FROM stock_prices\", conn_sqlite)\n",
        "        conn_sqlite.close()\n",
        "    elif local_max_date <= remote_max_date:\n",
        "        print(\"âœ… åˆ¤æ–·ï¼šSupabase å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€ä¸Šå‚³\")\n",
        "        upload_df = None\n",
        "    else:\n",
        "        print(f\"ğŸ“¤ åˆ¤æ–·ï¼šæœ‰æ–°è³‡æ–™ï¼ä¸Šå‚³æ—¥æœŸ > '{remote_max_date}' çš„è¨˜éŒ„\")\n",
        "        conn_sqlite = sqlite3.connect(\"local_taiwan_stock.db\")\n",
        "        upload_df = pd.read_sql(\n",
        "            \"SELECT * FROM stock_prices WHERE date > ?\",\n",
        "            conn_sqlite,\n",
        "            params=[remote_max_date]\n",
        "        )\n",
        "        conn_sqlite.close()\n",
        "        print(f\"ğŸ“Š å…±æ‰¾åˆ° {len(upload_df):,} ç­†æ–°è³‡æ–™\")\n",
        "\n",
        "    # 4. åŸ·è¡Œä¸Šå‚³ï¼ˆå¦‚æœæœ‰è³‡æ–™ï¼‰\n",
        "    if upload_df is not None and len(upload_df) > 0:\n",
        "        print(f\"ğŸš€ é–‹å§‹ä¸Šå‚³ {len(upload_df):,} ç­†è³‡æ–™åˆ° Supabase...\")\n",
        "        upload_df.to_sql(\n",
        "            name='stock_prices',\n",
        "            con=engine,\n",
        "            if_exists='append',\n",
        "            index=False,\n",
        "            chunksize=10000,\n",
        "            method='multi'\n",
        "        )\n",
        "        print(\"âœ… ä¸Šå‚³æˆåŠŸï¼\")\n",
        "    elif upload_df is not None:\n",
        "        print(\"â„¹ï¸ ç„¡æ–°è³‡æ–™éœ€è¦ä¸Šå‚³ã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ åŒæ­¥å¤±æ•—: {e}\")\n",
        "\n",
        "finally:\n",
        "    engine.dispose()\n",
        "    print(\"ğŸ”Œ è³‡æ–™åº«é€£ç·šå·²é—œé–‰ã€‚\")"
      ],
      "metadata": {
        "id": "ovok8cGrbhlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ğŸ“ˆ [Cell 3] å°è‚¡ã€Œé‚„åŸè‚¡åƒ¹ã€å…¨é »ç‡èšåˆæ¸…æ´—æµç¨‹ï¼ˆé€±/æœˆ/å¹´ Kï¼‰\n",
        "# =============================================================================\n",
        "# ğŸ” åŠŸèƒ½èªªæ˜ï¼š\n",
        "#   - å¾ Supabase è³‡æ–™åº«å–å¾—æ‰€æœ‰æ›¾ç™¼å¸ƒæœˆç‡Ÿæ”¶çš„è‚¡ç¥¨ä»£è™Ÿï¼ˆå«ä¸Šå¸‚ .TW / ä¸Šæ«ƒ .TWOï¼‰\n",
        "#   - ä½¿ç”¨ yfinance æŠ“å–è¿‘ 5 å¹´æ—¥é »è³‡æ–™ï¼ˆå« Adj Closeï¼‰\n",
        "#   - åŸ·è¡Œã€Œå°ˆæ¥­ç´šè‚¡åƒ¹æ¸…æ´—ã€ï¼šåµæ¸¬ä¸¦å‰”é™¤ã€Œä¹’ä¹“æ¥µç«¯éœ‡ç›ªã€ï¼ˆå¦‚æ¸›è³‡ã€ä½µè³¼ç•°å¸¸æ³¢å‹•ï¼‰\n",
        "#   - èšåˆç”Ÿæˆä¸‰å¼µæ¨™æº–åŒ– K ç·šè¡¨ï¼š\n",
        "#       â€¢ stock_annual_kï¼šå¹´åº¦é–‹/æ”¶ç›¤ï¼ˆç”¨æ–¼è¨ˆç®—çœŸå¯¦å¹´å ±é…¬ï¼‰\n",
        "#       â€¢ stock_monthly_kï¼šæœˆåº¦é–‹/æ”¶ç›¤ï¼ˆå°é½Šæ°‘åœ‹å¹´æœˆæ ¼å¼ï¼Œå¯ä¸²æ¥ç‡Ÿæ”¶ï¼‰\n",
        "#       â€¢ stock_weekly_kï¼šé€±ç·šæ”¶ç›¤ï¼ˆç”¨æ–¼å…¬å‘Šæ™‚åºäº‹ä»¶ç ”ç©¶ï¼‰\n",
        "#\n",
        "# âš™ï¸ æ ¸å¿ƒæŠ€è¡“äº®é»ï¼š\n",
        "#   âœ… ä½¿ç”¨ Adj Close è¨ˆç®—æ¼²è·Œå¹… â†’ è‡ªå‹•è™•ç†é™¤æ¬Šæ¯ã€æ¸›è³‡ã€ä½µè‚¡ç­‰äº‹ä»¶\n",
        "#   âœ… ã€Œä¹’ä¹“éæ¿¾ã€æ©Ÿåˆ¶ï¼šè‹¥é€£çºŒå…©æ—¥æ¼²è·Œå¹… >40% ä¸”æ–¹å‘ç›¸åï¼Œè¦–ç‚ºç•°å¸¸ä¸¦å‰”é™¤\n",
        "#        ï¼ˆæœ‰æ•ˆæ’é™¤å¦‚ 8476 å°å¢ƒç­‰æ¸›è³‡è‚¡çš„è™›å‡æš´æ¼²ï¼‰\n",
        "#   âœ… æœˆ K å°é½Šã€Œæ°‘åœ‹å¹´æœˆã€æ ¼å¼ï¼ˆe.g., \"113_01\"ï¼‰ï¼Œèˆ‡ monthly_revenue è¡¨ç„¡ç¸« JOIN\n",
        "#   âœ… å¹´ K ä¸ç›´æ¥ç”¨ yfinance å¹´é »ï¼Œè€Œæ˜¯ç”±æ—¥é »èšåˆ â†’ ç¢ºä¿ Adj Close é‚è¼¯ä¸€è‡´\n",
        "#   âœ… é€± K ä»ä½¿ç”¨ yfinance çš„ 1wk é »ç‡ï¼ˆç¬¦åˆäº¤æ˜“æ‰€é€±å®šç¾©ï¼‰ï¼Œå†ç¶“åŒæ¨£æ¸…æ´—\n",
        "#\n",
        "# ğŸŒ è³‡æ–™ä¾†æºèˆ‡é€£ç·šï¼š\n",
        "#   - è‚¡ç¥¨æ¸…å–®ï¼šSupabase PostgreSQL (monthly_revenue è¡¨)\n",
        "#   - è‚¡åƒ¹ä¾†æºï¼šYahoo Finance (yfinance)ï¼Œæ¶µè“‹ .TW / .TWO\n",
        "#   - å¯«å›ç›®æ¨™ï¼šåŒä¸€ Supabase è³‡æ–™åº«ï¼ˆè¦†è“‹ stock_*_k è¡¨ï¼‰\n",
        "#\n",
        "# âš ï¸ æ³¨æ„äº‹é …ï¼š\n",
        "#   - yfinance å¯èƒ½å¶æœ‰ç¼ºå¤±ï¼ˆå°¤å…¶å†·é–€æ«ƒè²·è‚¡ï¼‰ï¼Œè…³æœ¬æœƒè‡ªå‹•è·³éå¤±æ•—æ¨™çš„\n",
        "#   - PINGPONG_THRESHOLD = 0.40 æ˜¯ç¶“é©—å€¼ï¼Œå¯ä¾éœ€æ±‚èª¿æ•´ï¼ˆå¤ªä½æœƒèª¤åˆªï¼Œå¤ªé«˜æœƒæ¼æ¿¾ï¼‰\n",
        "#   - æ­¤æµç¨‹ã€Œä¸ä¿ç•™åŸå§‹æœªæ¸…æ´—è‚¡åƒ¹ã€ï¼Œåƒ…è¼¸å‡ºä¹¾æ·¨å¾Œçš„é‚„åŸåƒ¹æ ¼\n",
        "#   - è‹¥éœ€ã€Œæˆäº¤é‡ã€æˆ–ã€Œæœ€é«˜/æœ€ä½åƒ¹ã€ï¼Œå¯æ“´å±• agg å‡½æ•¸ï¼ˆç›®å‰èšç„¦é–‹/æ”¶ç›¤ï¼‰\n",
        "#\n",
        "# ğŸ’¡ åˆ†æåƒ¹å€¼ï¼š\n",
        "#   - çœŸå¯¦å¹´å ±é…¬ = year_close / year_open - 1ï¼ˆå·²æ ¡æ­£å…¬å¸è¡Œç‚ºï¼‰\n",
        "#   - å¯èˆ‡æœˆç‡Ÿæ”¶ YoY æ­é…ï¼Œè¨ˆç®—ã€Œç‡Ÿæ”¶æˆé•· vs è‚¡åƒ¹è¡¨ç¾ã€æ•£ä½ˆåœ–\n",
        "#   - é€±ç·šå¯ç”¨æ–¼æª¢é©—ã€Œç‡Ÿæ”¶å…¬å‘Šå¾Œ 1 é€±æ˜¯å¦è·‘è´å¤§ç›¤ã€\n",
        "#\n",
        "# ğŸ“Œ ä½œè€…ï¼šQuant Research Team | æœ€å¾Œæ›´æ–°ï¼š2025 å¹´\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "import urllib.parse\n",
        "from datetime import datetime\n",
        "\n",
        "# --- 1. é…ç½®å€ ---\n",
        "DB_PASSWORD = \"Supabase Service Role å¯†ç¢¼\"          # ä½ çš„ Supabase Service Role å¯†ç¢¼\n",
        "PROJECT_REF = \"å°ˆæ¡ˆ IDï¼ˆpostgres.<PROJECT_REF>ï¼‰\"     # å°ˆæ¡ˆ IDï¼ˆpostgres.<PROJECT_REF>ï¼‰\n",
        "POOLER_HOST = \"aws-1-ap-southeast-1.pooler.supabase.com\"\n",
        "connection_string = f\"postgresql://postgres.{PROJECT_REF}:{urllib.parse.quote_plus(DB_PASSWORD)}@{POOLER_HOST}:5432/postgres\"\n",
        "engine = create_engine(connection_string)\n",
        "\n",
        "# å°ˆæ¥­æ¸…æ´—é–€æª»\n",
        "PINGPONG_THRESHOLD = 0.40\n",
        "\n",
        "# --- 2. å°ˆæ¥­æ¸…æ´—å‡½æ•¸ ---\n",
        "def clean_k_data(df):\n",
        "    if df.empty: return df\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # ä½¿ç”¨ Adj Close ä¾†è¨ˆç®—çœŸå¯¦æ¼²è·Œå¹…ï¼Œé¿é–‹æ¸›è³‡/é™¤æ¬Šæ¯èª¤åˆ¤\n",
        "    df['pct_change'] = df['Adj Close'].pct_change()\n",
        "\n",
        "    mask_pingpong = pd.Series(False, index=df.index)\n",
        "    for i in range(1, len(df)-1):\n",
        "        prev = df.loc[i, 'pct_change']\n",
        "        nxt = df.loc[i+1, 'pct_change']\n",
        "        if pd.notna(prev) and pd.notna(nxt):\n",
        "            # åµæ¸¬æ¥µç«¯éœ‡ç›ª (ä¹’ä¹“)\n",
        "            if abs(prev) > PINGPONG_THRESHOLD and abs(nxt) > PINGPONG_THRESHOLD and prev * nxt < 0:\n",
        "                mask_pingpong.iloc[i] = True\n",
        "                mask_pingpong.iloc[i+1] = True\n",
        "\n",
        "    return df[~mask_pingpong].dropna(subset=['Adj Close']).copy()\n",
        "\n",
        "# --- 3. åŸ·è¡ŒæŠ“å– ---\n",
        "with engine.connect() as conn:\n",
        "    # é€™è£¡æˆ‘å€‘å¾ç‡Ÿæ”¶è¡¨æŠ“æ‰€æœ‰å‡ºç¾éçš„è‚¡ç¥¨ï¼Œç¢ºä¿å®Œæ•´æ€§\n",
        "    symbols = pd.read_sql(\"SELECT DISTINCT stock_id FROM monthly_revenue\", conn)['stock_id'].tolist()\n",
        "    # åŠ ä¸Šå°è‚¡å¾Œç¶´\n",
        "    symbols = [f\"{s}.TW\" if len(s)==4 else f\"{s}.TWO\" for s in symbols]\n",
        "\n",
        "print(f\"ğŸš€ é–‹å§‹åŸ·è¡Œã€é‚„åŸè‚¡åƒ¹ã€å…¨é »ç‡æ¸…æ´— (å‘¨/æœˆ/å¹´ K)...\")\n",
        "\n",
        "all_weekly, all_monthly, all_annual = [], [], []\n",
        "\n",
        "for i, sym in enumerate(symbols):\n",
        "    try:\n",
        "        # ä¸‹è¼‰æ•¸æ“š (åŒ…å« Adj Close)\n",
        "        # å¹´Kæ”¹ç”±æ—¥Kèšåˆä»¥ç²å¾—æœ€ç²¾ç¢ºçš„ Adj Close è½‰æ›\n",
        "        raw_data = yf.download(sym, period=\"5y\", interval=\"1d\", progress=False)\n",
        "\n",
        "        if raw_data.empty: continue\n",
        "\n",
        "        # 1. åŸºç¤æ¸…æ´—\n",
        "        df_clean = clean_k_data(raw_data.reset_index())\n",
        "        df_clean['symbol'] = sym\n",
        "\n",
        "        # 2. è™•ç†å¹´ K (å¹´åº¦å ±é…¬æœ€é—œéµï¼šä½¿ç”¨é‚„åŸè‚¡åƒ¹)\n",
        "        df_clean['year_str'] = df_clean['Date'].dt.year.astype(str)\n",
        "        annual_summary = df_clean.groupby('year_str').agg(\n",
        "            year_open=('Adj Close', 'first'),\n",
        "            year_close=('Adj Close', 'last')\n",
        "        ).reset_index()\n",
        "        annual_summary.columns = ['year', 'year_open', 'year_close']\n",
        "        annual_summary['symbol'] = sym\n",
        "        all_annual.append(annual_summary)\n",
        "\n",
        "        # 3. è™•ç†æœˆ K (å°é½Šç‡Ÿæ”¶æœˆä»½)\n",
        "        df_clean['report_month'] = df_clean['Date'].apply(lambda x: f\"{x.year-1911}_{x.month:02d}\")\n",
        "        monthly_summary = df_clean.groupby('report_month').agg(\n",
        "            m_open=('Adj Close', 'first'),\n",
        "            m_close=('Adj Close', 'last')\n",
        "        ).reset_index()\n",
        "        monthly_summary['symbol'] = sym\n",
        "        all_monthly.append(monthly_summary)\n",
        "\n",
        "        # 4. è™•ç†å‘¨ K (ç”¨æ–¼å…¬å‘Šæ™‚åºç ”ç©¶)\n",
        "        # å‘¨ç·šæˆ‘å€‘é‡æ–°æŠ“å– 1wk é »ç‡ä»¥å°é½Šæ¨™æº–å‘¨å®šç¾©\n",
        "        raw_w = yf.download(sym, period=\"5y\", interval=\"1wk\", progress=False)\n",
        "        if not raw_w.empty:\n",
        "            df_w = clean_k_data(raw_w.reset_index())\n",
        "            df_w['symbol'] = sym\n",
        "            df_w = df_w[['Date', 'symbol', 'Adj Close']] # ç°¡åŒ–ï¼Œä¸»è¦çœ‹é‚„åŸå¾Œçš„æ”¶ç›¤\n",
        "            df_w.columns = ['date', 'symbol', 'w_close']\n",
        "            all_weekly.append(df_w)\n",
        "\n",
        "        if (i+1) % 50 == 0:\n",
        "            print(f\"âœ… å·²è™•ç† {i+1} æª” ({(i+1)/len(symbols)*100:.1f}%)\")\n",
        "\n",
        "    except Exception: continue\n",
        "\n",
        "# --- 4. è¦†è“‹è³‡æ–™åº« ---\n",
        "if all_annual:\n",
        "    pd.concat(all_annual).to_sql('stock_annual_k', engine, if_exists='replace', index=False)\n",
        "if all_monthly:\n",
        "    pd.concat(all_monthly).to_sql('stock_monthly_k', engine, if_exists='replace', index=False)\n",
        "if all_weekly:\n",
        "    pd.concat(all_weekly).to_sql('stock_weekly_k', engine, if_exists='replace', index=False)\n",
        "\n",
        "print(\"\\nâœ¨ æ•¸æ“šå¤§æ¸…æ´—å®Œæˆï¼å°å¢ƒ (8476) ç­‰æ¸›è³‡è‚¡çš„æ¼²å¹…å·²å›æ­¸çœŸå¯¦ã€‚\")"
      ],
      "metadata": {
        "id": "4qQJe_WUdBbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ğŸ“Œ ç›®æ¨™ï¼šå»ºç«‹ã€Œå‘¨Kç·šè³‡æ–™è¡¨ã€stock_weekly_kï¼ˆå¾ stock_daily_k èšåˆï¼‰\n",
        "# ğŸ“Œ è¼¸å‡ºæ¬„ä½ï¼šdate, symbol, w_open, w_close, w_high, w_low\n",
        "# ğŸ“Œ æ³¨æ„ï¼šä¸åŒ…å« volumeï¼›é€±æœ«å®šç¾©ç‚ºã€Œæ¯é€±äº”ã€ï¼ˆè‹¥ç„¡äº¤æ˜“å‰‡å–è©²é€±æœ€å¾Œäº¤æ˜“æ—¥ï¼‰\n",
        "# =============================================================================\n",
        "\n",
        "# 1ï¸âƒ£ å®‰è£å¿…è¦å¥—ä»¶ï¼ˆåƒ…é¦–æ¬¡åŸ·è¡Œéœ€è¦ï¼‰\n",
        "!pip install sqlalchemy psycopg2-binary pandas -q\n",
        "\n",
        "# 2ï¸âƒ£ åŒ¯å…¥æ¨¡çµ„\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "import urllib.parse\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ” Supabase é€£ç·šè¨­å®šï¼ˆè«‹ç¢ºèªæ­£ç¢ºï¼‰\n",
        "# =============================================================================\n",
        "DB_PASSWORD = \"Supabase Service Role å¯†ç¢¼\"          # ä½ çš„ Supabase Service Role å¯†ç¢¼\n",
        "PROJECT_REF = \"å°ˆæ¡ˆ IDï¼ˆpostgres.<PROJECT_REF>ï¼‰\"     # å°ˆæ¡ˆ IDï¼ˆpostgres.<PROJECT_REF>ï¼‰\n",
        "POOLER_HOST = \"aws-1-ap-southeast-1.pooler.supabase.com\"\n",
        "\n",
        "encoded_password = urllib.parse.quote_plus(DB_PASSWORD)\n",
        "connection_string = (\n",
        "    f\"postgresql://postgres.{PROJECT_REF}:{encoded_password}\"\n",
        "    f\"@{POOLER_HOST}:5432/postgres?sslmode=require\"\n",
        ")\n",
        "engine = create_engine(connection_string)\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ” ç¬¬ä¸€æ­¥ï¼šç¢ºèªä¾†æºè¡¨ stock_daily_k æ˜¯å¦å­˜åœ¨\n",
        "# =============================================================================\n",
        "def table_exists(table_name):\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(text(\"\"\"\n",
        "            SELECT EXISTS (\n",
        "                SELECT FROM information_schema.tables\n",
        "                WHERE table_schema = 'public' AND table_name = :name\n",
        "            );\n",
        "        \"\"\"), {\"name\": table_name}).scalar()\n",
        "        return result\n",
        "\n",
        "if not table_exists(\"stock_daily_k\"):\n",
        "    print(\"âŒ éŒ¯èª¤ï¼šä¾†æºè¡¨ 'stock_daily_k' ä¸å­˜åœ¨ï¼\")\n",
        "    print(\"   è«‹å…ˆç¢ºä¿æœ‰æ—¥Kè³‡æ–™è¡¨ï¼Œæ¬„ä½åŒ…å«ï¼šdate, symbol, open, high, low, close\")\n",
        "else:\n",
        "    print(\"âœ… æ‰¾åˆ°ä¾†æºè¡¨ stock_daily_kï¼Œé–‹å§‹å»ºç«‹å‘¨K...\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # ğŸ› ï¸ ç¬¬äºŒæ­¥ï¼šå»ºç«‹ stock_weekly_kï¼ˆä½œç‚º Tableï¼Œé Viewï¼‰\n",
        "    # ä½¿ç”¨ CREATE TABLE AS èªæ³•ä¸€æ¬¡æ€§å¯«å…¥æ‰€æœ‰å‘¨Kè³‡æ–™\n",
        "    # =============================================================================\n",
        "    create_weekly_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS stock_weekly_k AS\n",
        "    WITH weekly_groups AS (\n",
        "        SELECT\n",
        "            symbol,\n",
        "            date,\n",
        "            open,\n",
        "            high,\n",
        "            low,\n",
        "            close,\n",
        "            -- å°‡æ—¥æœŸæ­¸å±¬åˆ°ã€Œç•¶é€±äº”ã€ï¼ˆISO é€±ï¼šé€±ä¸€=1, é€±æ—¥=7ï¼‰\n",
        "            -- DATE_TRUNC('week', ...) å›å‚³é€±ä¸€ï¼Œ+6å¤© â†’ é€±æ—¥ï¼›ä½†æˆ‘å€‘è¦é€±äº” â†’ +4\n",
        "            DATE_TRUNC('week', date)::DATE + 4 AS week_end\n",
        "        FROM stock_daily_k\n",
        "        WHERE date IS NOT NULL\n",
        "          AND symbol IS NOT NULL\n",
        "    ),\n",
        "    weekly_agg AS (\n",
        "        SELECT\n",
        "            symbol,\n",
        "            week_end AS date,\n",
        "            FIRST_VALUE(open) OVER w AS w_open,\n",
        "            LAST_VALUE(close) OVER w AS w_close,\n",
        "            MAX(high) OVER w AS w_high,\n",
        "            MIN(low) OVER w AS w_low\n",
        "        FROM weekly_groups\n",
        "        WINDOW w AS (\n",
        "            PARTITION BY symbol, week_end\n",
        "            ORDER BY date\n",
        "            ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
        "        )\n",
        "    )\n",
        "    SELECT DISTINCT\n",
        "        date,\n",
        "        symbol,\n",
        "        w_open,\n",
        "        w_close,\n",
        "        w_high,\n",
        "        w_low\n",
        "    FROM weekly_agg\n",
        "    ORDER BY symbol, date;\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        with engine.begin() as conn:\n",
        "            # å¦‚æœå·²å­˜åœ¨ï¼Œå…ˆåˆªé™¤ï¼ˆé¿å…é‡è¤‡è³‡æ–™ï¼‰\n",
        "            conn.execute(text(\"DROP TABLE IF EXISTS stock_weekly_k;\"))\n",
        "            conn.execute(text(create_weekly_sql))\n",
        "        print(\"âœ¨ æˆåŠŸå»ºç«‹è³‡æ–™è¡¨ï¼šstock_weekly_k\")\n",
        "\n",
        "        # =============================================================================\n",
        "        # ğŸ” ç¬¬ä¸‰æ­¥ï¼šé¡¯ç¤ºå‰ 5 ç­†é©—è­‰\n",
        "        # =============================================================================\n",
        "        with engine.connect() as conn:\n",
        "            result = conn.execute(text(\"SELECT * FROM stock_weekly_k LIMIT 5;\"))\n",
        "            df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
        "        print(\"\\nğŸ“Š é©—è­‰çµæœï¼ˆå‰ 5 ç­†ï¼‰:\")\n",
        "        print(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ å»ºç«‹å¤±æ•—: {e}\")\n",
        "        raise\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ’¡ èªªæ˜ï¼š\n",
        "# - æ¯é€±ä»¥ã€Œé€±äº”ã€ç‚ºä»£è¡¨æ—¥ï¼ˆå³ä½¿ç•¶å¤©ç„¡äº¤æ˜“ï¼Œä¹Ÿæœƒç”¨è©²é€±æœ€å¾Œäº¤æ˜“æ—¥ï¼‰\n",
        "# - w_open = è©²é€±ç¬¬ä¸€ç­†æ—¥Kçš„é–‹ç›¤åƒ¹\n",
        "# - w_close = è©²é€±æœ€å¾Œç­†æ—¥Kçš„æ”¶ç›¤åƒ¹\n",
        "# - w_high / w_low = è©²é€±æœ€é«˜/æœ€ä½åƒ¹\n",
        "# - è‹¥æœªä¾†æœ‰æ–°æ—¥Kè³‡æ–™ï¼Œéœ€æ‰‹å‹•é‡æ–°åŸ·è¡Œæ­¤è…³æœ¬æ›´æ–°å‘¨K\n",
        "# ============================================================================="
      ],
      "metadata": {
        "id": "uYiJGWXYsWKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ğŸ“Œ ç›®æ¨™ï¼šåœ¨ Supabase ä¸­å»ºç«‹ã€Œå¹´Kç·šè¦–åœ–ã€stock_annual_k\n",
        "# ğŸ“Œ è¼¸å…¥ä¾†æºï¼šstock_weekly_kï¼ˆå‘¨Kè³‡æ–™è¡¨ï¼‰\n",
        "# ğŸ“Œ è¼¸å‡ºæ¬„ä½ï¼šsymbol, year, first/last trade date, year_open/close/high/low/volume\n",
        "# ğŸ“Œ æ³¨æ„ï¼šè‹¥ stock_weekly_k æ²’æœ‰ w_volumeï¼Œæœƒè‡ªå‹•è¨­ç‚º NULL\n",
        "# =============================================================================\n",
        "\n",
        "# 1ï¸âƒ£ å®‰è£å¿…è¦å¥—ä»¶ï¼ˆåƒ…é¦–æ¬¡åŸ·è¡Œéœ€è¦ï¼‰\n",
        "!pip install sqlalchemy psycopg2-binary pandas -q\n",
        "\n",
        "# 2ï¸âƒ£ åŒ¯å…¥æ‰€éœ€æ¨¡çµ„\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "import urllib.parse\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ” è³‡æ–™åº«é€£ç·šè¨­å®šï¼ˆè«‹ç¢ºèªå¯†ç¢¼èˆ‡å°ˆæ¡ˆ ID æ­£ç¢ºï¼‰\n",
        "# =============================================================================\n",
        "DB_PASSWORD = \"Supabase Service Role å¯†ç¢¼\"          # ä½ çš„ Supabase Service Role å¯†ç¢¼\n",
        "PROJECT_REF = \"å°ˆæ¡ˆ IDï¼ˆpostgres.<PROJECT_REF>ï¼‰\"     # å°ˆæ¡ˆ IDï¼ˆpostgres.<PROJECT_REF>ï¼‰\n",
        "POOLER_HOST = \"aws-1-ap-southeast-1.pooler.supabase.com\"\n",
        "\n",
        "# URL ç·¨ç¢¼å¯†ç¢¼ï¼ˆè™•ç† '+' ç­‰ç‰¹æ®Šå­—å…ƒï¼‰\n",
        "encoded_password = urllib.parse.quote_plus(DB_PASSWORD)\n",
        "connection_string = (\n",
        "    f\"postgresql://postgres.{PROJECT_REF}:{encoded_password}\"\n",
        "    f\"@{POOLER_HOST}:5432/postgres?sslmode=require\"\n",
        ")\n",
        "\n",
        "# å»ºç«‹ SQLAlchemy å¼•æ“\n",
        "engine = create_engine(connection_string)\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ” ç¬¬ä¸€æ­¥ï¼šæª¢æŸ¥ stock_weekly_k æ˜¯å¦æœ‰ w_volume æ¬„ä½\n",
        "# =============================================================================\n",
        "def check_column_exists(table_name, column_name):\n",
        "    with engine.connect() as conn:\n",
        "        query = text(\"\"\"\n",
        "            SELECT EXISTS (\n",
        "                SELECT 1 FROM information_schema.columns\n",
        "                WHERE table_name = :table AND column_name = :col\n",
        "            );\n",
        "        \"\"\")\n",
        "        return conn.execute(query, {\"table\": table_name, \"col\": column_name}).scalar()\n",
        "\n",
        "has_volume = check_column_exists(\"stock_weekly_k\", \"w_volume\")\n",
        "print(f\"âœ… stock_weekly_k æ˜¯å¦åŒ…å« w_volume æ¬„ä½ï¼Ÿ{'æ˜¯' if has_volume else 'å¦'}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ› ï¸ ç¬¬äºŒæ­¥ï¼šç”Ÿæˆ CREATE VIEW SQL èªæ³•ï¼ˆæ ¹æ“šæ˜¯å¦æœ‰ volume å‹•æ…‹èª¿æ•´ï¼‰\n",
        "# =============================================================================\n",
        "if has_volume:\n",
        "    volume_expr = \"SUM(w_volume) AS year_volume\"\n",
        "else:\n",
        "    volume_expr = \"NULL::NUMERIC AS year_volume  -- åŸå§‹è³‡æ–™ç„¡æˆäº¤é‡æ¬„ä½\"\n",
        "\n",
        "create_view_sql = f\"\"\"\n",
        "CREATE OR REPLACE VIEW stock_annual_k AS\n",
        "SELECT DISTINCT\n",
        "    symbol,\n",
        "    EXTRACT(YEAR FROM date)::TEXT AS year,\n",
        "    MIN(date) OVER yr AS first_trade_date,\n",
        "    MAX(date) OVER yr AS last_trade_date,\n",
        "    FIRST_VALUE(w_open) OVER yr AS year_open,\n",
        "    LAST_VALUE(w_close) OVER yr AS year_close,\n",
        "    MAX(w_high) OVER yr AS year_high,\n",
        "    MIN(w_low) OVER yr AS year_low,\n",
        "    {volume_expr}\n",
        "FROM stock_weekly_k\n",
        "WINDOW yr AS (\n",
        "    PARTITION BY symbol, EXTRACT(YEAR FROM date)\n",
        "    ORDER BY date\n",
        "    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# âš™ï¸ ç¬¬ä¸‰æ­¥ï¼šåŸ·è¡Œ CREATE VIEW\n",
        "# =============================================================================\n",
        "try:\n",
        "    with engine.begin() as conn:  # è‡ªå‹• commit æˆ– rollback\n",
        "        conn.execute(text(create_view_sql))\n",
        "    print(\"\\nâœ¨ æˆåŠŸå»ºç«‹æˆ–æ›´æ–° View: stock_annual_k\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ å»ºç«‹ View å¤±æ•—: {e}\")\n",
        "    raise\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ” ç¬¬å››æ­¥ï¼šæŸ¥è©¢å‰ 5 ç­†è³‡æ–™é©—è­‰çµæœ\n",
        "# =============================================================================\n",
        "try:\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(text(\"SELECT * FROM stock_annual_k LIMIT 5;\"))\n",
        "        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
        "    print(\"\\nğŸ“Š é©—è­‰çµæœï¼ˆstock_annual_k å‰ 5 ç­†ï¼‰:\")\n",
        "    print(df)\n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸ æŸ¥è©¢é©—è­‰å¤±æ•—ï¼ˆå¯èƒ½å› ç„¡è³‡æ–™ï¼‰: {e}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ’¡ ä½¿ç”¨èªªæ˜ï¼š\n",
        "# 1. æ­¤ View æœƒè‡ªå‹•å¾ stock_weekly_k èšåˆå‡ºå¹´åº¦ K ç·š\n",
        "# 2. year_open = è©²å¹´ç¬¬ä¸€ç­†å‘¨Kçš„é–‹ç›¤åƒ¹\n",
        "# 3. year_close = è©²å¹´æœ€å¾Œç­†å‘¨Kçš„æ”¶ç›¤åƒ¹\n",
        "# 4. year_high/low = è©²å¹´æ‰€æœ‰å‘¨Kçš„æœ€é«˜/æœ€ä½åƒ¹\n",
        "# 5. è‹¥ç„¡ w_volumeï¼Œyear_volume å°‡ç‚º NULL\n",
        "# ============================================================================="
      ],
      "metadata": {
        "id": "6RjVTk5isXKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}